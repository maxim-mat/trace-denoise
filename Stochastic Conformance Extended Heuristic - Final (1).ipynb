{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-30T15:53:14.439774500Z",
     "start_time": "2024-08-30T15:53:14.406275800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:85% !important; }</style>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import deque\n",
    "import io\n",
    "import copy \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.optimize import linprog\n",
    "from heapq import heapify, heappush, heappop\n",
    "import pm4py \n",
    "import random\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments\n",
    "import warnings\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "import pulp\n",
    "from pulp import *\n",
    "import pickle\n",
    "import torch \n",
    "import sympy\n",
    "from random import sample\n",
    "# from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.filterwarnings(action='ignore', category=Warning, module='pandas')\n",
    "# warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "np.set_printoptions(linewidth=np.inf)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-30T15:53:14.489774500Z",
     "start_time": "2024-08-30T15:53:14.410774500Z"
    }
   },
   "outputs": [],
   "source": [
    "class Place:\n",
    "    def __init__(self, name, in_arcs=None, out_arcs=None, properties={}):\n",
    "        self.name = name\n",
    "        self.in_arcs = set() if in_arcs is None else in_arcs\n",
    "        self.out_arcs = set() if out_arcs is None else out_arcs\n",
    "        self.properties = properties\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "    \n",
    "    \n",
    "class Transition:\n",
    "    def __init__(self, name, label, in_arcs=None, out_arcs=None, move_type=None, prob=None, weight=None, location_idx=None,\n",
    "                 cost_function=None, properties={}):\n",
    "        self.name = name\n",
    "        self.label = label\n",
    "        self.in_arcs = set() if in_arcs is None else in_arcs \n",
    "        self.out_arcs = set() if out_arcs is None else out_arcs\n",
    "        self.move_type = move_type\n",
    "        self.prob = prob\n",
    "        self.cost_function = cost_function\n",
    "        self.weight = self.__initialize_weight(weight)\n",
    "        self.properties = properties\n",
    "        self.location_idx=location_idx\n",
    "\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "    \n",
    "    def __initialize_weight(self, weight):\n",
    "        if weight is not None:\n",
    "            return weight\n",
    "        \n",
    "        if self.prob == 0:\n",
    "            return np.inf\n",
    "        \n",
    "        if self.cost_function is None:\n",
    "            return 0 if self.move_type == 'sync' else 1\n",
    "            # raise Exception(\"A cost function should be defined for transitions if no weight is given and prob is positive\")\n",
    "        return self.cost_function(self.prob)\n",
    "        \n",
    "    \n",
    "class Arc:\n",
    "    def __init__(self, source, target, weight=1, properties={}):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.weight = weight\n",
    "        self.properties = properties\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.source.name + ' -> ' + self.target.name \n",
    "    \n",
    "    \n",
    "class Marking:\n",
    "    def __init__(self, places=None):\n",
    "        self.places = tuple(0 for place in places) if places is None else places\n",
    "    def __repr__(self):\n",
    "        return str(self.places)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.places == other.places\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.places)\n",
    "    \n",
    "    \n",
    "class Node:\n",
    "    def __init__(self, marking):\n",
    "        self.marking = marking\n",
    "        self.neighbors = set()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.marking)\n",
    "    \n",
    "    def add_neighbor(self, node, transition):\n",
    "        self.neighbors.add((node, transition)) \n",
    "        \n",
    "        \n",
    "class Edge:\n",
    "    def __init__(self, name, source_marking, target_marking, move_type):\n",
    "        self.name = name\n",
    "        self.source_marking = source_marking\n",
    "        self.target_marking = target_marking\n",
    "        self.move_type = move_type\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self.source_marking} -> {self.name} -> {self.target_marking}'\n",
    "    \n",
    "    \n",
    "class Graph:\n",
    "    def __init__(self, nodes = None, edges = None, starting_node = None, ending_node = None):\n",
    "        self.nodes = set() if nodes is None else nodes\n",
    "        self.edges = set() if edges is None else edges\n",
    "        self.starting_node = starting_node\n",
    "        self.ending_node = ending_node\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'Nodes:{self.nodes}, \\n edges:{self.edges}'\n",
    "    \n",
    "    def __get_markings(self):\n",
    "        return set([node.marking for node in self.nodes])\n",
    "    \n",
    "    def add_node(self, node):\n",
    "        self.nodes.add(node)\n",
    "        \n",
    "    def add_edge(self, edge):\n",
    "        self.edges.add(edge)\n",
    "        \n",
    "        \n",
    "# class search_node:\n",
    "#     def __init__(self, graph_node, dist=np.inf, ancestor=None, transition_to_ancestor=None):\n",
    "#         self.graph_node = graph_node\n",
    "#         self.dist = dist\n",
    "#         self.ancestor = ancestor\n",
    "#         self.transition_to_ancestor = transition_to_ancestor\n",
    "\n",
    "#     def __lt__(self, other):\n",
    "#         return self.dist <= other.dist\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return f'Node: {self.graph_node.marking}, dist:{self.dist}'\n",
    "\n",
    "\n",
    "# class search_node:\n",
    "    \n",
    "#     def __init__(self, ancestor, transition_to_ancestor, marking, dist_from_origin, herustic_to_end):\n",
    "#         self.ancestor = ancestor\n",
    "#         self.transition_to_ancestor = transition_to_ancestor\n",
    "#         self.marking = marking\n",
    "#         self.dist_from_origin = dist_from_origin\n",
    "#         self.herustic_to_end = herustic_to_end\n",
    "#         self.total_distance = dist_from_origin + herustic_to_end\n",
    "        \n",
    "        \n",
    "#     def __lt__(self, other):\n",
    "#         return self.total_distance < other.total_distance\n",
    " \n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return f'Node: {self.marking}, dist:{self.dist_from_origin}'\n",
    "\n",
    "\n",
    "class search_node:\n",
    "    \n",
    "    def __init__(self, ancestor, transition_to_ancestor, marking, dist_from_origin, solution_vec=None, heuristic_distance=np.inf,\n",
    "                                   have_exact_known_solution=False, have_estimated_solution=False, n_explained_events=0, disappointing=None):\n",
    "        \n",
    "        self.ancestor = ancestor\n",
    "        self.transition_to_ancestor = transition_to_ancestor\n",
    "        self.marking = marking\n",
    "        self.dist_from_origin = dist_from_origin\n",
    "        self.heuristic_distance = heuristic_distance\n",
    "        self.total_distance = dist_from_origin + heuristic_distance\n",
    "        self.solution_vec = solution_vec\n",
    "        self.have_exact_known_solution = have_exact_known_solution\n",
    "        self.have_estimated_solution = have_estimated_solution\n",
    "        self.n_explained_events = n_explained_events\n",
    "        self.disappointing = disappointing\n",
    "        \n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        \n",
    "        if self.disappointing is None:\n",
    "            if self.have_exact_known_solution != other.have_exact_known_solution:\n",
    "                return self.have_exact_known_solution > other.have_exact_known_solution\n",
    "        \n",
    "            if self.solution_vec is not None and other.solution_vec is not None:\n",
    "                if self.total_distance == other.total_distance:\n",
    "                    return sum(self.solution_vec) < sum(other.solution_vec)\n",
    "        \n",
    "            return self.total_distance < other.total_distance\n",
    "\n",
    "        else: \n",
    "            return self.total_distance < other.total_distance\n",
    "        \n",
    "        \n",
    "#     def __lt__(self, other):\n",
    "        \n",
    "#         if round(self.total_distance, 3) != round(other.total_distance, 3):\n",
    "#             return self.total_distance < other.total_distance\n",
    "        \n",
    "#         if self.have_exact_known_solution != other.have_exact_known_solution:\n",
    "#             return self.have_exact_known_solution > other.have_exact_known_solution\n",
    "        \n",
    "#         if self.solution_vec is not None and other.solution_vec is not None:\n",
    "#             return sum(self.solution_vec) < sum(other.solution_vec)\n",
    "            \n",
    " \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Node: {self.marking}, dist:{self.dist_from_origin}'\n",
    "    \n",
    "    \n",
    "class PetriNet:\n",
    "    def __init__(self, name, places=None, transitions=None, arcs=None, trace_len=None, properties={}):\n",
    "        self.name = name\n",
    "        self.transitions = list() if transitions is None else transitions\n",
    "        self.places = list() if places is None else places\n",
    "        self.arcs = list() if arcs is None else arcs\n",
    "        self.properties = properties\n",
    "        self.init_mark = None\n",
    "        self.final_mark = None\n",
    "        self.reachability_graph = None\n",
    "        self.places_indices = {} if places is None else {self.places[i].name:i for i in range(len(self.places))}\n",
    "        self.transitions_indices = {} if transitions is None else {self.transitions[i].name:i for i in range(len(self.transitions))}\n",
    "        self.dis_associated_indices = None \n",
    "        self.trace_transitions = None\n",
    "        self.transitions_weights = list() if transitions is None else np.array([t.weight for t in transitions])\n",
    "        self.trace_len = trace_len\n",
    "        self.cost_function = None\n",
    "        \n",
    "        \n",
    "    def update_transitions_weights(self):\n",
    "        transitions_weights = np.array([t.weight for t in self.transitions])\n",
    "        \n",
    "        return transitions_weights\n",
    "       \n",
    "        \n",
    "#     def compute_dis_associated_indices_for_heuristic(self):\n",
    "#         transition_labels_set = {transition.label for transition in self.transitions if transition.move_type == 'trace'}\n",
    "#         label_indices_dict = {}\n",
    "        \n",
    "#         for label in transition_labels_set:\n",
    "#             dis_associated_indices = [idx for idx, transition in enumerate(self.transitions) if transition.label != label or \n",
    "#                                      transition.label == label and transition.move_type =='model']\n",
    "    \n",
    "#             label_indices_dict[label] = dis_associated_indices\n",
    "        \n",
    "#         print(f'number of possible indices for each transition label within the trace: {[len(self.transitions)-len(label_indices_dict[label]) for label in label_indices_dict.keys()]}')\n",
    "#         return label_indices_dict\n",
    "    \n",
    "\n",
    "    def compute_conditioned_weight(self, path_prefix, transition, prob_dict, lamda=0.5):\n",
    "\n",
    "        if prob_dict is None:\n",
    "            return transition.weight \n",
    "        \n",
    "        # print(f'prob dict = {prob_dict}')\n",
    "        if transition.label is None:\n",
    "#             print(f'Transition label=None thus returning weight of 0') \n",
    "            return 0\n",
    "        \n",
    "#         print(f'original trans weight={transition.weight}')  \n",
    "        transition_weight = transition.weight\n",
    "        transition_label = transition.label\n",
    "        full_path = path_prefix+transition_label\n",
    "#         print(f'The full path including the transition label={full_path}')\n",
    "        \n",
    "        if path_prefix == '':\n",
    "            return transition_weight\n",
    "        \n",
    "        if full_path in prob_dict:\n",
    "#             print(f'full path={full_path} is in the prob dict and conditioned weight= {0.5*(1-prob_dict[full_path]) + 0.5*transition_weight}')\n",
    "#             return (1-lamda)*(1-prob_dict[full_path]) + lamda*transition_weight\n",
    "            return (1-lamda)*((1-prob_dict[full_path])*transition_weight) + lamda*transition_weight\n",
    "        \n",
    "#         print(f'full path={full_path} is not in the prob dict.. sorry.. ')\n",
    "        longest_prefix = self.find_longest_prefix(full_path, prob_dict)\n",
    "        \n",
    "        if longest_prefix:\n",
    "#             print(f'longest prefix={longest_prefix} is in dict! The conditioned weight= {0.5*(1-prob_dict[longest_prefix]) + 0.5*transition_weight}')\n",
    "#             return (1-lamda)*(1-prob_dict[longest_prefix]) + lamda*transition_weight\n",
    "            return (1-lamda)*((1-prob_dict[longest_prefix])*transition_weight) + lamda*transition_weight        \n",
    "#         print(f'no prefix exists for {full_path}..conditioned weight= {0.5 + 0.5*transition_weight}')\n",
    "#         return (1-lamda) + lamda*transition_weight\n",
    "        return transition_weight    \n",
    "    \n",
    "    def find_longest_prefix(self, full_path, prob_dict):\n",
    "        longest_prefix = None\n",
    "        for i in range(len(full_path)-1):\n",
    "            if full_path[i:] in prob_dict:\n",
    "                return full_path[i:]\n",
    "#             print(f'prefix={full_path[i:]} is not in the dict')\n",
    "            \n",
    "        return longest_prefix    \n",
    "    \n",
    "    def compute_disassosiate_indices_for_heuristic(self):\n",
    "        disassociate_indices_dict = defaultdict(list)\n",
    "        for i in range(self.trace_len):\n",
    "            disassociate_indices = [idx for idx, transition in enumerate(self.transitions) if transition.location_idx != i]\n",
    "            disassociate_indices_dict[i] = disassociate_indices   \n",
    "        return disassociate_indices_dict\n",
    "    \n",
    "    \n",
    "    def add_places(self, places):\n",
    "        if isinstance(places, list):\n",
    "            self.places += places\n",
    "        \n",
    "        else:\n",
    "            self.places.append(places)\n",
    "        \n",
    "        self.__update_indices_p_dict(places)\n",
    "     \n",
    "    \n",
    "    def _find_available_transitions(self, mark_tuple):\n",
    "        '''Input: tuple\n",
    "           Output: list'''\n",
    "        \n",
    "        available_transitions = []\n",
    "        for transition in self.transitions:\n",
    "            if self.__check_transition_prerequesits(transition, mark_tuple):\n",
    "                available_transitions.append(transition)\n",
    "                \n",
    "        return available_transitions  \n",
    "\n",
    "    \n",
    "    def _fire_transition(self, mark, transition):\n",
    "        '''Input: Mark object, Transition object\n",
    "        Output: Marking object''' \n",
    "        \n",
    "        subtract_mark = [0] * len(mark.places)\n",
    "        for arc in transition.in_arcs:\n",
    "            place_idx = self.places_indices[arc.source.name]\n",
    "            subtract_mark[place_idx] -= arc.weight\n",
    "        \n",
    "        add_mark = [0] * len(mark.places)\n",
    "        for arc in transition.out_arcs:\n",
    "            place_idx = self.places_indices[arc.target.name]\n",
    "            add_mark[place_idx] += arc.weight\n",
    "  \n",
    "        new_mark = tuple([sum(x) for x in zip(list(mark.places), subtract_mark, add_mark)])\n",
    "        for elem in new_mark:\n",
    "            if elem < 0:\n",
    "                print(f'the mark was: {mark} and I subtract the following values: {subtract_mark} and adding these: {add_mark} \\\n",
    "                which results in this: {new_mark} and all this sh!t was by using this transition: {transition.name}')\n",
    "        new_mark_obj = Marking(new_mark)\n",
    "        \n",
    "        return new_mark_obj\n",
    "    \n",
    "    \n",
    "    def add_transitions(self, transitions):\n",
    "        if isinstance(transitions, list):\n",
    "            self.transitions += transitions\n",
    "        \n",
    "        else:\n",
    "            self.transitions.append(transitions)\n",
    "        \n",
    "        self.__update_indices_t_dict(transitions)\n",
    "#         self.dis_associated_indices = self.compute_disassosiate_indices_for_heuristic()\n",
    "        self.transitions_weights = self.update_transitions_weights()\n",
    "        \n",
    "        \n",
    "    def compute_heuristic(self, incidence_mat, curr_marking):\n",
    "        curr_marking_vec = self.convert_marking_to_np_vector(curr_marking)\n",
    "        final_marking_vec = self.convert_marking_to_np_vector(self.final_mark)\n",
    "        obj = self.transitions_weights\n",
    "        rhs_eq = final_marking_vec - curr_marking_vec \n",
    "        opt = linprog(c=obj, A_eq=incidence_mat, b_eq=rhs_eq, method=\"revised simplex\")\n",
    "\n",
    "        if opt['success'] == True:\n",
    "#             print(f'scipy heuristic is a success! heuristic distance={opt.fun}')\n",
    "            \n",
    "            return opt.fun, opt.x\n",
    "        \n",
    "        print('problems in heuristic.. no solution!!!!')\n",
    "        return float('inf'), None\n",
    "    \n",
    " \n",
    " #    def astar(self):\n",
    "#         # PROBLEM!!!! - NO heapify implemented and no update to nodes which were found by a shorter path!!!\n",
    "#         distance_min_heap = []\n",
    "#         heapify(distance_min_heap)\n",
    "#         init_node = search_node(None, None, self.init_mark, 0, 0)\n",
    "#         heappush(distance_min_heap, init_node)\n",
    "#         incidence_mat = self.compute_incidence_matrix()\n",
    "        \n",
    "#         while distance_min_heap:\n",
    "#             min_dist_node = heappop(distance_min_heap)\n",
    "#             print(f'min node has total distance of {min_dist_node.total_distance}')\n",
    "#             if min_dist_node.marking == self.final_mark:\n",
    "#                 print('final marking has been reached!!')\n",
    "#                 print(f'n_nodes left in heap: {len(distance_min_heap)}')\n",
    "#                 break\n",
    "\n",
    "#             for transition in self.__find_available_transitions(min_dist_node.marking.places):\n",
    "#                 new_mark = self.__fire_transition(min_dist_node.marking, transition)\n",
    "#                 heuristic_distance = self.compute_heuristic(incidence_mat, new_mark)\n",
    "#                 new_node = search_node(min_dist_node, transition, new_mark, \n",
    "#                                        min_dist_node.dist_from_origin + transition.weight, heuristic_distance) \n",
    "#                 heappush(distance_min_heap, new_node)\n",
    "\n",
    "#         curr_node = min_dist_node\n",
    "#         path = []\n",
    "\n",
    "#         while curr_node.ancestor:           \n",
    "#             path.append(curr_node.transition_to_ancestor.name)\n",
    "#             curr_node = curr_node.ancestor\n",
    "\n",
    "#         return path[::-1], min_dist_node.dist_from_origin\n",
    "    \n",
    "    \n",
    "    def initialize_min_dist_node(self, k, incidence_mat, consump_mat):\n",
    "        heuristic_distance, sol_vec = self.compute_heuristic_extended(k, self.transitions_weights,\n",
    "                                                                      np.array(self.init_mark.places),\n",
    "                                                                      np.array(self.final_mark.places),\n",
    "                                                                      incidence_mat, consump_mat)                                                                      \n",
    "    \n",
    "        init_node = search_node(ancestor=None, transition_to_ancestor=None, marking=self.init_mark, dist_from_origin=0, solution_vec=sol_vec,\n",
    "                                heuristic_distance=heuristic_distance, have_exact_known_solution=True)\n",
    "#         print(f'The initial starting node is initializd with the following values given k_values={k}: heuristic_distance={init_node.heuristic_distance}, total distance: {init_node.total_distance}')\n",
    "        return init_node\n",
    " \n",
    "    \n",
    "    def astar_extended(self, k=None, s=0):\n",
    "        if k is None:\n",
    "            k = set()\n",
    "#         print(f'starting the search algorithm.. the k set is: {k}')\n",
    "        \n",
    "        visited_markings = set()\n",
    "        visited_markings_distance_dict = {}        \n",
    "        distance_min_heap = []\n",
    "        heapify(distance_min_heap)        \n",
    "        # these two should be new attributes in the petri net instead of copying them each time\n",
    "        incidence_mat = self.compute_incidence_matrix() # Need to check the condition (t,p) not in F\n",
    "#         print(f' sum of incidence mat values = {np.sum(incidence_mat)}')\n",
    "#         print(f'number of non zero elems in incidence mat: {np.count_nonzero(incidence_mat)}')\n",
    "        consump_mat = self.compute_consumption_matrix()\n",
    "#         print(f' consump mat sum: {np.sum(consump_mat)}')\n",
    "        init_node = self.initialize_min_dist_node(k, incidence_mat, consump_mat)\n",
    "        heappush(distance_min_heap, init_node)\n",
    "        visited_markings_distance_dict[self.init_mark] = init_node\n",
    "        final_min_dist=np.inf\n",
    "        node=None\n",
    "        \n",
    "        while distance_min_heap:\n",
    "            need_heapify = False\n",
    "            min_dist_node = heappop(distance_min_heap) \n",
    "\n",
    "#             print(f'The min dist node has distance so far={round(min_dist_node.dist_from_origin, 12)}, estimated_distance={min_dist_node.heuristic_distance}, have_exact_known_solution={min_dist_node.have_exact_known_solution}, have_estimated_solution={min_dist_node.have_estimated_solution}')\n",
    "#             if min_dist_node.have_exact_known_solution:\n",
    "#                 print([(self.transitions[idx].name, round(num,12)) for idx, num in enumerate(min_dist_node.solution_vec) if num>0])\n",
    "#             else:\n",
    "#                 print('min dist node does not have an exact solution')\n",
    "#             print(f'The heap has the following (except the min one which was popped) nodes within (node_total_distance, have_exact_sol, have_estimated_sol) \\n:{[(round(node.total_distance,2), node.have_exact_known_solution, node.have_estimated_solution) for node in distance_min_heap]}')\n",
    "\n",
    "            if min_dist_node.marking == self.final_mark:\n",
    "                break\n",
    "            \n",
    "            if min_dist_node.have_estimated_solution:\n",
    "                max_events_explained = max(min_dist_node.n_explained_events, s)\n",
    "                if max_events_explained not in k:\n",
    "                    k.add(max_events_explained)\n",
    "                    return self.astar_extended(k, s=0)\n",
    "\n",
    "\n",
    "#                 heuristic_distance_test, sol_vec_test = self.compute_heuristic(incidence_mat, min_dist_node.marking)\n",
    "#                 temp_node = min_dist_node\n",
    "#                 temp_path = []\n",
    "\n",
    "#                 while temp_node.ancestor:           \n",
    "#                     temp_path.append(temp_node.transition_to_ancestor.name)\n",
    "#                     temp_node = temp_node.ancestor\n",
    "                    \n",
    "                heuristic_distance, sol_vec = self.compute_heuristic_extended(None, self.transitions_weights,\n",
    "                                                                      np.array(min_dist_node.marking.places),\n",
    "                                                                      np.array(self.final_mark.places),\n",
    "                                                                      incidence_mat, consump_mat, init_node_comp=False,\n",
    "                                                                      n_explained_events= min_dist_node.n_explained_events) \n",
    "#                 print(f'path to the node:{temp_path[::-1]}')\n",
    "#                 print(f'returning from mid-run heuristic values heuristic_distance={heuristic_distance} compared to previous estimation of {min_dist_node.heuristic_distance} and sol_vec={[(self.transitions[idx].name, round(num,3), round(self.transitions[idx].weight,3)) for idx, num in enumerate(sol_vec) if num>0]}')\n",
    "#                 if round(heuristic_distance_test) != round(heuristic_distance):\n",
    "#                     raise ValueError(f'heuristic_distance_test={heuristic_distance_test}, heuristic_distance={heuristic_distance}')\n",
    "                    \n",
    "                min_dist_node.disappointing = True\n",
    "        \n",
    "                if sol_vec is not None:  \n",
    "#                     print('sol vec is not none')\n",
    "                    min_dist_node.have_exact_known_solution = True\n",
    "                    min_dist_node.have_estimated_solution = False\n",
    "                    min_dist_node.solution_vec = sol_vec\n",
    "                \n",
    "                else:\n",
    "#                     print('sol vec is none!!!!!!!')\n",
    "                    min_dist_node.heuristic_distance = np.inf\n",
    "                    min_dist_node.total_distance = min_dist_node.dist_from_origin + min_dist_node.heuristic_distance\n",
    "                    min_dist_node.solution_vec = None\n",
    "                    heappush(distance_min_heap, min_dist_node) \n",
    "                    continue \n",
    "                    \n",
    "                if heuristic_distance > min_dist_node.heuristic_distance:\n",
    "                    min_dist_node.heuristic_distance = heuristic_distance\n",
    "                    min_dist_node.total_distance = min_dist_node.dist_from_origin + min_dist_node.heuristic_distance\n",
    "                    heappush(distance_min_heap, min_dist_node) \n",
    "                    continue\n",
    "                    \n",
    "                ## new ##\n",
    "#                 else:\n",
    "#                     min_dist_node.heuristic_distance = heuristic_distance                    \n",
    "#                     min_dist_node.total_distance = min_dist_node.dist_from_origin + min_dist_node.heuristic_distance                    \n",
    "               ##########\n",
    "            \n",
    "            s = max(s, min_dist_node.n_explained_events)\n",
    "            visited_markings.add(min_dist_node.marking)            \n",
    "\n",
    "            for transition in self.__find_available_transitions(min_dist_node.marking.places):\n",
    "#                 print(f'currently exploring tranisition= {transition.name}, weight={transition.weight}')\n",
    "                new_mark = self.__fire_transition(min_dist_node.marking, transition)   \n",
    "                need_push_node = False    \n",
    "                transition_idx = self.transitions_indices[transition.name]  \n",
    "                \n",
    "                if new_mark not in visited_markings:\n",
    "                    dist_to_node = min_dist_node.dist_from_origin + transition.weight\n",
    "                    sol_vec = np.array(min_dist_node.solution_vec)\n",
    "                    \n",
    "                    if new_mark in visited_markings_distance_dict:\n",
    "                        if((dist_to_node > visited_markings_distance_dict[new_mark].dist_from_origin) or (dist_to_node == visited_markings_distance_dict[new_mark].dist_from_origin and sol_vec[transition_idx] < 0.999)): \n",
    "#                             if dist_to_node > visited_markings_distance_dict[new_mark].dist_from_origin:\n",
    "#                                 print(f'distance to node through transition {transition} is {dist_to_node} but current distance is {visited_markings_distance_dict[new_mark].dist_from_origin} and thus no update')\n",
    "#                             if dist_to_node == visited_markings_distance_dict[new_mark].dist_from_origin and sol_vec[transition_idx] < 0.999:\n",
    "#                                 print(f'same distance from both paths {dist_to_node} = {visited_markings_distance_dict[new_mark].dist_from_origin} but no heuristic since value of sol vel in idx is {sol_vec[transition_idx]}')\n",
    "                            continue\n",
    "                    \n",
    "                    if new_mark not in visited_markings_distance_dict:\n",
    "                        need_push_node = True\n",
    "                        node = search_node(min_dist_node, transition, new_mark, dist_to_node)\n",
    "                        visited_markings_distance_dict[new_mark] = node \n",
    "                            \n",
    "                        \n",
    "                    else:\n",
    "#                         print(f'the marking already exists but using transition {transition.name} with the value of {visited_markings_distance_dict[new_mark].dist_from_origin} results in a shorted way to the marking thus updating the markings distance to: {dist_to_node} and the new dady transition is {transition.name}')\n",
    "#                         print('Taking existing node and updating its value')\n",
    "                        need_heapify = True\n",
    "                        node = visited_markings_distance_dict[new_mark]\n",
    "                        node.dist_from_origin = dist_to_node\n",
    "                        node.ancestor = min_dist_node\n",
    "                        node.transition_to_ancestor = transition\n",
    "#                         print(f'number of markings within dict={len(visited_markings_distance_dict)} while number of unique nodes is:{len(set([id(value) for value in visited_markings_distance_dict.values()]))}')              \n",
    "                    node.heuristic_distance = max(0, min_dist_node.heuristic_distance - transition.weight)  \n",
    "                    node.total_distance = node.heuristic_distance + node.dist_from_origin\n",
    "\n",
    "\n",
    "#                     need_push_node = True\n",
    "#                     node = search_node(min_dist_node, transition, new_mark, dist_to_node) # This line sohuld be checked!!!!!\n",
    "#                     visited_markings_distance_dict[new_mark] = node \n",
    "#                     node.heuristic_distance = max(0, min_dist_node.heuristic_distance - transition.weight)  \n",
    "#                     node.total_distance = node.heuristic_distance + node.dist_from_origin\n",
    "\n",
    "                    if min_dist_node.solution_vec[transition_idx] >= 0.999:\n",
    "#                         print(f'since transition {transition.name}, idx={transition_idx} have value bigger than 1 within the heuristic we reuse existing sol vec')\n",
    "                        new_sol_vec = np.array(min_dist_node.solution_vec, copy=True)\n",
    "                        if min_dist_node.solution_vec[transition_idx] >= 1:\n",
    "                            new_sol_vec[transition_idx] -= 1\n",
    "                        else:\n",
    "                            new_sol_vec[transition_idx] = 0\n",
    "                        node.solution_vec = new_sol_vec\n",
    "#                         print(f'the new sol vec is: {[(self.transitions[idx].name, round(num,2)) for idx, num in enumerate(new_sol_vec) if num>0]}')\n",
    "                        node.have_exact_known_solution = True\n",
    "                        node.have_estimated_solution = False\n",
    "                    \n",
    "                    else:\n",
    "#                         print(f'transition {transition.name} with idx={transition_idx} does not appear within the solution vec thus no sol vec and estimated_sol=True')\n",
    "#                         print(f'the value of tranisiton={transition.name} with idx={transition_idx} inside the sol vector is={min_dist_node.solution_vec[transition_idx]}')\n",
    "#                         print(f'Here is the solution vector where the transition does not appear in: \\n {[(self.transitions[idx].name, round(num,12), idx) for idx, num in enumerate(min_dist_node.solution_vec) if num>0]}')\n",
    "                        node.have_exact_known_solution = False\n",
    "                        node.have_estimated_solution = True  \n",
    "                        node.solution_vec = None\n",
    "                    \n",
    "                    node.disappointing = min_dist_node.disappointing\n",
    "                    \n",
    "                    if transition.move_type in {'sync', 'trace'}:\n",
    "                        node.n_explained_events = min_dist_node.n_explained_events + 1\n",
    "                    \n",
    "                    else:\n",
    "                        node.n_explained_events = min_dist_node.n_explained_events\n",
    "                    \n",
    "#                     print(f'the value of need_push={need_push_node}')\n",
    "                    if need_push_node:\n",
    "#                         print('pushing node inside heap!!')\n",
    "                        heappush(distance_min_heap, node)\n",
    "#                     print(f'after updating the node the new values are: distance_from_origin={node.dist_from_origin}, heuristic_value={node.heuristic_distance}, total_distance={node.total_distance}')  \n",
    "            if need_heapify:\n",
    "                heapify(distance_min_heap)\n",
    "            \n",
    "#             print(f'node daddy id before pop:{id(node.ancestor)}')\n",
    "#             daddy_id_before = id(node.ancestor)\n",
    "    \n",
    "        curr_node = min_dist_node\n",
    "        path = []\n",
    "\n",
    "        while curr_node.ancestor:           \n",
    "            path.append(curr_node.transition_to_ancestor.name)\n",
    "            curr_node = curr_node.ancestor\n",
    "        \n",
    "        print(f'Optimal alignment cost: {min_dist_node.dist_from_origin }') # , \\n Optimal alignment: \\n {path[::-1]}')\n",
    "        return path[::-1], min_dist_node.dist_from_origin                    \n",
    "                    \n",
    "                    \n",
    "            \n",
    "    def compute_heuristic_extended(self, k_set, c, m_i, m_f, incidence_mat, consump_mat, init_node_comp=True, n_explained_events=None):\n",
    "#         print('Entering extended heuristic...')\n",
    "        if self.dis_associated_indices is None:\n",
    "            self.dis_associated_indices = self.compute_disassosiate_indices_for_heuristic()\n",
    "#             print([len(self.dis_associated_indices[key]) for key in self.dis_associated_indices.keys()])\n",
    "        \n",
    "        if k_set is None:\n",
    "            k_set = set()\n",
    "            \n",
    "        model = LpProblem(\"Heuristic-Estimator\", LpMinimize)\n",
    "        if init_node_comp:\n",
    "            n_ys = len(k_set) + 1\n",
    "            n_xs = n_ys + 1\n",
    "\n",
    "            X_mat = np.array([str(i)+ '_' + str(j) for i in range(n_xs) for j in range(1,incidence_mat.shape[1]+1)]) \n",
    "            Y_mat = np.array([str(i)+ '_' + str(j) for i in range(1, n_ys+1) for j in range(1,incidence_mat.shape[1]+1)])\n",
    "\n",
    "            X_variables = LpVariable.matrix(\"X\", X_mat, lowBound= 0) # constraint 3:  cat='Integer' \n",
    "            Y_variables = LpVariable.matrix(\"Y\", Y_mat, lowBound= 0) # constrainrt 4: cat='Binary'\n",
    "\n",
    "            allocation_x = np.array(X_variables).reshape(n_xs, incidence_mat.shape[1])    \n",
    "            allocation_y = np.array(Y_variables).reshape(n_ys, incidence_mat.shape[1])  \n",
    "\n",
    "\n",
    "            # Objective Function\n",
    "            obj_func = lpSum(c@allocation_x.T) + lpSum(c@allocation_y.T)\n",
    "            model +=  obj_func\n",
    "\n",
    "\n",
    "            # Constraint 1\n",
    "            total_allocation = allocation_x.T.sum(axis=1) + allocation_y.T.sum(axis=1)\n",
    "            for i in range(incidence_mat.shape[0]):\n",
    "                model += lpSum([m_i[i], incidence_mat[i,:]@total_allocation]) == m_f[i]\n",
    "\n",
    "\n",
    "            # Constraint 2 \n",
    "            for j in range(1, n_xs):\n",
    "                curr_alloc_x = allocation_x[:j,:]\n",
    "                curr_alloc_y = allocation_y[:j-1,:] if j!=1 else np.zeros(allocation_y.shape[1]).reshape(-1,1)\n",
    "                curr_total_allocation = curr_alloc_x.T.sum(axis=1) + curr_alloc_y.T.sum(axis=1)\n",
    "                curr_alloc_y_j = allocation_y[j-1,:] \n",
    "\n",
    "                for i in range(incidence_mat.shape[0]):\n",
    "                    model += lpSum([m_i[i], incidence_mat[i,:]@curr_total_allocation, consump_mat[i,:]@curr_alloc_y_j]) >= 0         \n",
    "\n",
    "\n",
    "            # Constraint 5 \n",
    "            for row_idx, trace_location_idx in enumerate(sorted(list(k_set))):\n",
    "                model += lpSum(allocation_y[row_idx+1][i] for i in self.dis_associated_indices[trace_location_idx]) == 0\n",
    "            model += lpSum(allocation_y[0][i] for i in self.dis_associated_indices[0]) == 0   \n",
    "\n",
    "            # Constraint 6\n",
    "            ones_vec = np.ones(incidence_mat.shape[1])\n",
    "            for row_idx in range(len(allocation_y)):\n",
    "                model += lpSum(ones_vec@allocation_y[row_idx].T) == 1\n",
    "\n",
    "\n",
    "                \n",
    "        else:    \n",
    "            n_xs = 1\n",
    "            X_mat = np.array([str(i)+ '_' + str(j) for i in range(n_xs) for j in range(1,incidence_mat.shape[1]+1)])             \n",
    "            X_variables = LpVariable.matrix(\"X\", X_mat, lowBound= 0)          \n",
    "            allocation_x = np.array(X_variables).reshape(n_xs, incidence_mat.shape[1])      \n",
    "           \n",
    "            obj_func = lpSum(c@allocation_x.T)\n",
    "            model +=  obj_func        \n",
    "        \n",
    "            total_allocation = allocation_x.T.sum(axis=1)\n",
    "            for i in range(incidence_mat.shape[0]):\n",
    "                model += lpSum([m_i[i], incidence_mat[i,:]@total_allocation]) == m_f[i]        \n",
    "        \n",
    "        \n",
    "         \n",
    "        # Solving the optimization problem and returning the results\n",
    "        model.solve(PULP_CBC_CMD());\n",
    "        if init_node_comp:\n",
    "            x_res = np.array([val.value() for val in X_variables]).reshape(n_xs, incidence_mat.shape[1]).T\n",
    "            y_res = np.array([val.value() for val in Y_variables]).reshape(n_ys, incidence_mat.shape[1]).T\n",
    "            sol_vec = x_res.sum(axis=1) + y_res.sum(axis=1)\n",
    "            \n",
    "        else:\n",
    "            x_res = np.array([val.value() for val in X_variables]).reshape(n_xs, incidence_mat.shape[1]).T\n",
    "            sol_vec = x_res.sum(axis=1)\n",
    "            \n",
    "        heuristic_distance = model.objective.value()\n",
    "        \n",
    "#         print(f'LP status: {LpStatus[model.status]}')\n",
    "        \n",
    "        if LpStatus[model.status] != 'Optimal':\n",
    "            print(f'returning inf distance and None vector')\n",
    "            return np.inf, None\n",
    "        \n",
    "        return heuristic_distance, sol_vec\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def compute_incidence_matrix(self):\n",
    "        length = len(self.places)\n",
    "        width = len(self.transitions)\n",
    "        inc_mat = np.zeros((length, width))\n",
    "        \n",
    "        for arc in self.arcs:\n",
    "            if isinstance(arc.source, Place):\n",
    "                i, j = self.places_indices[arc.source.name],  self.transitions_indices[arc.target.name]\n",
    "                inc_mat[i,j] -= 1\n",
    "\n",
    "            else:\n",
    "                j, i = self.transitions_indices[arc.source.name],  self.places_indices[arc.target.name]\n",
    "                inc_mat[i,j] += 1\n",
    "\n",
    "        return inc_mat\n",
    "\n",
    "    \n",
    "    def compute_consumption_matrix(self):\n",
    "        length = len(self.places)\n",
    "        width = len(self.transitions)\n",
    "        consump_mat = np.zeros((length, width))        \n",
    "\n",
    "        for arc in self.arcs:\n",
    "            if isinstance(arc.source, Place):\n",
    "                i, j = self.places_indices[arc.source.name],  self.transitions_indices[arc.target.name]\n",
    "                consump_mat[i,j] = -1 \n",
    "\n",
    "        return consump_mat\n",
    "    \n",
    "    \n",
    "    def generate_location_dict(self, iterable):\n",
    "        return {item.name:idx for idx, item in enumerate(iterable)}\n",
    "        \n",
    "    \n",
    "    \n",
    "    def generate_nonsync_arcs_for_sync_product(self, model_places, model_transitions, trace_places, trace_transitions,\n",
    "                                               trace_model):\n",
    "\n",
    "        model_p_dict = self.generate_location_dict(model_places)\n",
    "        model_t_dict = self.generate_location_dict(model_transitions)\n",
    "\n",
    "        trace_p_dict = self.generate_location_dict(trace_places)\n",
    "        trace_t_dict = self.generate_location_dict(trace_transitions)\n",
    "\n",
    "        model_arcs = [Arc(model_places[model_p_dict[a.source.name]], model_transitions[model_t_dict[a.target.name]]) \n",
    "                                 if type(a.source) == Place else Arc(model_transitions[model_t_dict[a.source.name]],\n",
    "                                 model_places[model_p_dict[a.target.name]]) for a in self.arcs]\n",
    "\n",
    "        trace_arcs = [Arc(trace_places[trace_p_dict[a.source.name]], trace_transitions[trace_t_dict[a.target.name]]) \n",
    "                             if type(a.source) == Place else Arc(trace_transitions[trace_t_dict[a.source.name]],\n",
    "                             trace_places[trace_p_dict[a.target.name]]) for a in trace_model.arcs]\n",
    "\n",
    "        return model_arcs, trace_arcs\n",
    "\n",
    "\n",
    "    def convert_marking_to_np_vector(self, marking):\n",
    "        return np.array(marking.places)\n",
    "    \n",
    "      \n",
    "    def assign_non_sync_arcs_to_transitions(self, model_places, trace_places, model_transitions, trace_transitions,\n",
    "                                            model_arcs, trace_arcs):\n",
    "        \n",
    "        model_p_dict = self.generate_location_dict(model_places)\n",
    "        model_t_dict = self.generate_location_dict(model_transitions)\n",
    "        trace_p_dict = self.generate_location_dict(trace_places)\n",
    "        trace_t_dict = self.generate_location_dict(trace_transitions)\n",
    "        \n",
    "        for arc in model_arcs:\n",
    "            if type(arc.source) == Place:\n",
    "                p_idx, t_idx = model_p_dict[arc.source.name], model_t_dict[arc.target.name]\n",
    "                model_places[p_idx].out_arcs.add(arc)\n",
    "                model_transitions[t_idx].in_arcs.add(arc)\n",
    "                \n",
    "            else:\n",
    "                t_idx, p_idx = model_t_dict[arc.source.name], model_p_dict[arc.target.name]\n",
    "                model_transitions[t_idx].out_arcs.add(arc)\n",
    "                model_places[p_idx].in_arcs.add(arc)\n",
    "                \n",
    "                \n",
    "        for arc in trace_arcs:\n",
    "            if type(arc.source) == Place:\n",
    "                p_idx, t_idx = trace_p_dict[arc.source.name], trace_t_dict[arc.target.name]\n",
    "                trace_places[p_idx].out_arcs.add(arc)\n",
    "                trace_transitions[t_idx].in_arcs.add(arc)\n",
    "                \n",
    "            else:\n",
    "                t_idx, p_idx = trace_t_dict[arc.source.name], trace_p_dict[arc.target.name]\n",
    "                trace_transitions[t_idx].out_arcs.add(arc)\n",
    "                trace_places[p_idx].in_arcs.add(arc)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def construct_reachability_graph(self):   \n",
    "        curr_mark = self.init_mark\n",
    "        curr_node = Node(curr_mark.places)\n",
    "        self.reachability_graph = Graph()\n",
    "        if self.final_mark is not None:\n",
    "            self.reachability_graph.ending_node = Node(self.final_mark.places)\n",
    "        self.reachability_graph.add_node(curr_node)\n",
    "        self.reachability_graph.starting_node = curr_node\n",
    "        available_transitions = self.__find_available_transitions(curr_mark.places)\n",
    "        nodes_to_explore = deque()\n",
    "        visited_marks = set()\n",
    "        \n",
    "        for transition in available_transitions:\n",
    "            nodes_to_explore.append((curr_mark, transition, curr_node))\n",
    "            \n",
    "        visited_marks.add(curr_mark.places)\n",
    "\n",
    "        while nodes_to_explore:\n",
    "            prev_node_triplet = nodes_to_explore.popleft()\n",
    "            prev_mark, prev_transition, prev_node = prev_node_triplet[0], prev_node_triplet[1], prev_node_triplet[2]\n",
    "            assert self.__check_transition_prerequesits(prev_transition, prev_mark.places) == True\n",
    "            curr_mark = self.__fire_transition(prev_mark, prev_transition)\n",
    "            curr_node = Node(curr_mark.places)\n",
    "            prev_node.add_neighbor(curr_node, prev_transition)\n",
    "            self.reachability_graph.add_edge(Edge(prev_transition.name, prev_mark, curr_mark, prev_transition.move_type))\n",
    "            \n",
    "            if curr_mark.places in visited_marks:\n",
    "                 continue\n",
    "            \n",
    "            else:\n",
    "                for transition in self.__find_available_transitions(curr_mark.places):\n",
    "                    nodes_to_explore.append((curr_mark, transition, curr_node))\n",
    "                        \n",
    "                visited_marks.add(curr_mark.places) \n",
    "                self.reachability_graph.add_node(curr_node)\n",
    "\n",
    "    \n",
    "    def construct_synchronous_product(self, trace_model):\n",
    "        '''This func assigns all trace transitions move_type=trace and all model transitions move_type=model\n",
    "        additionaly, all sync transitions will be assigned move_type=sync '''\n",
    "        \n",
    "        model_places = [Place(p.name) for p in self.places]\n",
    "        model_transitions = [Transition(t.name, t.label, move_type='model', prob=t.prob,\n",
    "                                             weight=t.weight) for t in self.transitions]\n",
    "        \n",
    "        trace_places = [Place(p.name) for p in trace_model.places]\n",
    "        trace_transitions = [Transition(t.name, t.label, move_type='trace', prob=t.prob,\n",
    "                                      weight=t.weight, location_idx=t.location_idx) for t in trace_model.transitions]\n",
    "        \n",
    "        \n",
    "        model_arcs, trace_arcs = self.generate_nonsync_arcs_for_sync_product(model_places, model_transitions,\n",
    "                                                                             trace_places, trace_transitions, trace_model)\n",
    "\n",
    "        self.assign_non_sync_arcs_to_transitions(model_places, trace_places, model_transitions, trace_transitions,\n",
    "                                                 model_arcs, trace_arcs)\n",
    "        \n",
    "        new_sync_transitions, new_sync_arcs = self.__generate_all_sync_transitions(model_transitions, trace_transitions)\n",
    "        \n",
    "        sync_model_all_places = model_places + trace_places \n",
    "        sync_model_all_transitions = model_transitions + trace_transitions + new_sync_transitions\n",
    "        sync_model_all_arcs = model_arcs + trace_arcs + new_sync_arcs\n",
    "        self.__update_sync_product_trans_names(sync_model_all_transitions)\n",
    "        \n",
    "        sync_prod = PetriNet('sync_prod', sync_model_all_places, sync_model_all_transitions, sync_model_all_arcs, trace_len=trace_model.trace_len)\n",
    "    \n",
    "        sync_prod.init_mark = Marking(self.init_mark.places + trace_model.init_mark.places)\n",
    "        sync_prod.final_mark = Marking(self.final_mark.places + trace_model.final_mark.places)\n",
    "        \n",
    "        sync_prod.trace_transitions = trace_transitions\n",
    "#         print(f'number of places={len(sync_prod.places)}, number of transitions={len(sync_prod.transitions)}, number of sync transitions={len(new_sync_transitions)} \\n')\n",
    "#         print(f'numebr of arcs: {len(sync_model_all_arcs)}, the arcs are: {[(arc.source.name, arc.target.name) for arc in sync_model_all_arcs]}')\n",
    "        return sync_prod\n",
    "        \n",
    "             \n",
    "    \n",
    "    def add_transitions_with_arcs(self, transitions):\n",
    "        if isinstance(transitions, list):\n",
    "            self.transitions += transitions\n",
    "            for transition in transitions:\n",
    "                self.arcs += list(transition.in_arcs.union(transition.out_arcs))\n",
    "\n",
    "        else:\n",
    "            self.transitions.append(transitions) \n",
    "            self.arcs += list(transition.in_arcs.union(transition.out_arcs))\n",
    "\n",
    "        self.__update_indices_t_dict(transitions)\n",
    "  \n",
    "\n",
    "    def add_arc_from_to(self, source, target, weight=None):\n",
    "            if weight is None:\n",
    "                arc = Arc(source, target)\n",
    "            else:\n",
    "                arc = Arc(source, target, weight)\n",
    "            source.out_arcs.add(arc)\n",
    "            target.in_arcs.add(arc)\n",
    "            self.arcs.append(arc)\n",
    "\n",
    "    \n",
    "#     def __generate_all_sync_transitions(self, trace_model):\n",
    "#         sync_transitions = []\n",
    "#         counter = 1\n",
    "\n",
    "#         for trans in self.transitions:\n",
    "#             # trans.label is guaranteed to be unique in the discovered model (from docs)\n",
    "#             if trans.label is not None:\n",
    "#                 # Find in the trace model all the transitions with the same label\n",
    "#                 same_label_transitions = self.__find_simillar_label_transitions(trace_model, trans.label)\n",
    "\n",
    "#                 for trace_trans in same_label_transitions:\n",
    "#                     new_sync_trans = self.__generate_new_trans(trans, trace_trans, counter)\n",
    "#                     sync_transitions.append(new_sync_trans)\n",
    "#                     counter += 1\n",
    "     \n",
    "#         return sync_transitions\n",
    "\n",
    "\n",
    "    def __generate_all_sync_transitions(self, model_transitions, trace_transitions):\n",
    "        sync_transitions = []\n",
    "        new_sync_arcs = []\n",
    "        \n",
    "        for model_tran in model_transitions:\n",
    "            # trans.label is guaranteed to be unique in the discovered model (from docs)\n",
    "            if model_tran.label is not None:\n",
    "                # Find in the trace model all the transitions with the same label\n",
    "                same_label_transitions = self.__find_simillar_label_transitions(trace_transitions, model_tran.label)\n",
    "\n",
    "                for trace_tran in same_label_transitions:\n",
    "                    new_sync_trans, new_arcs = self.__generate_new_trans(model_tran, trace_tran)\n",
    "                    sync_transitions.append(new_sync_trans)\n",
    "                    new_sync_arcs += new_arcs\n",
    "    \n",
    "        return sync_transitions, new_sync_arcs\n",
    "    \n",
    "    \n",
    "    def __find_simillar_label_transitions(self, trace_transitions, activity_label):\n",
    "        '''Returns all the transitions in the trace with a specified activity label'''\n",
    "        same_label_trans = [transition for transition in trace_transitions if transition.label == activity_label]\n",
    "                                                                                                   \n",
    "        return same_label_trans\n",
    "        \n",
    "           \n",
    "    def __generate_new_trans(self, model_tran, trace_tran):\n",
    "#         name = 'sync_transition_' + str(counter)\n",
    "        name = f'sync_{trace_tran.name}'\n",
    "        new_sync_transition = Transition(name=name, label=trace_tran.label, move_type='sync', prob=trace_tran.prob)\n",
    "        new_sync_transition.location_idx = trace_tran.location_idx\n",
    "        \n",
    "        input_arcs = model_tran.in_arcs.union(trace_tran.in_arcs)\n",
    "        new_input_arcs = []\n",
    "        for arc in input_arcs:\n",
    "            new_arc = Arc(arc.source, new_sync_transition, arc.weight)\n",
    "            new_input_arcs.append(new_arc)\n",
    "            \n",
    "        output_arcs = model_tran.out_arcs.union(trace_tran.out_arcs)\n",
    "        new_output_arcs = []\n",
    "        for arc in output_arcs:\n",
    "            new_arc = Arc(new_sync_transition, arc.target, arc.weight)\n",
    "            new_output_arcs.append(new_arc)\n",
    "       \n",
    "        new_sync_transition.in_arcs = new_sync_transition.in_arcs.union(new_input_arcs)\n",
    "        new_sync_transition.out_arcs = new_sync_transition.out_arcs.union(new_output_arcs)\n",
    "       \n",
    "        return new_sync_transition, new_input_arcs + new_output_arcs        \n",
    "\n",
    "    \n",
    "    def __update_indices_p_dict(self, places):\n",
    "        curr_idx = len(self.places_indices)\n",
    "        if isinstance(places, list):\n",
    "            for p in places:\n",
    "                self.places_indices[p.name] = curr_idx\n",
    "                curr_idx += 1\n",
    "        else:\n",
    "            self.places_indices[places.name] = curr_idx\n",
    "     \n",
    "    \n",
    "    def __update_indices_t_dict(self, transitions):\n",
    "        curr_idx = len(self.transitions_indices)\n",
    "        if isinstance(transitions, list):\n",
    "            for t in transitions:\n",
    "                self.transitions_indices[t.name] = curr_idx\n",
    "                curr_idx += 1\n",
    "        else:\n",
    "            self.transitions_indices[transitions.name] = curr_idx            \n",
    "     \n",
    "    \n",
    "    def __find_available_transitions(self, mark_tuple):\n",
    "        '''Input: tuple\n",
    "           Output: list'''\n",
    "        \n",
    "        available_transitions = []\n",
    "        for transition in self.transitions:\n",
    "            if self.__check_transition_prerequesits(transition, mark_tuple):\n",
    "                available_transitions.append(transition)\n",
    "                \n",
    "        return available_transitions\n",
    "\n",
    "    \n",
    "    def __check_transition_prerequesits(self, transition, mark_tuple):\n",
    "        for arc in transition.in_arcs:\n",
    "            arc_weight = arc.weight\n",
    "            source_idx = self.places_indices[arc.source.name]\n",
    "            if mark_tuple[source_idx] < arc_weight:\n",
    "                return False\n",
    "            \n",
    "        return True\n",
    "            \n",
    "    \n",
    "    def __assign_trace_transitions_move_type(self, trace_model):\n",
    "        trace_model_copy = copy.deepcopy(trace_model)\n",
    "        for trans in trace_model_copy.transitions:\n",
    "            if trans.move_type is None:\n",
    "                trans.move_type = 'trace'\n",
    "            \n",
    "        return trace_model_copy\n",
    "    \n",
    "    \n",
    "    def __assign_model_transitions_move_type(self):\n",
    "        for trans in self.transitions:\n",
    "            if trans.move_type is None:\n",
    "                trans.move_type = 'model'\n",
    "                \n",
    "\n",
    "#     def conformance_checking(self):\n",
    "#         if self.reachability_graph is None:\n",
    "#             self.construct_reachability_graph()\n",
    "            \n",
    "#         alignment, min_cost_distance = self.__dijkstra()\n",
    "#         return alignment, min_cost_distance\n",
    "\n",
    "#     def conformance_checking(self, trace_model):\n",
    "#         sync_prod = self.construct_synchronous_product(trace_model)\n",
    "# #         sync_prod.construct_reachability_graph()\n",
    "\n",
    "#         return sync_prod.astar_extended()\n",
    "\n",
    "\n",
    "    def conformance_checking(self, trace_model, hist_prob_dict=None, lamda=0.5):\n",
    "        sync_prod = self.construct_synchronous_product(trace_model)      \n",
    "        return sync_prod._dijkstra_no_rg_construct(hist_prob_dict, lamda=lamda)\n",
    "\n",
    "\n",
    "    def __dijkstra(self):\n",
    "        distance_min_heap = []\n",
    "        heapify(distance_min_heap)\n",
    "        visited_nodes = set()\n",
    "        search_graph_nodes = [search_node(node) for node in self.reachability_graph.nodes]\n",
    "        nodes_idx_dict = {search_node.graph_node.marking:idx for idx, search_node in enumerate(search_graph_nodes)}    \n",
    "        \n",
    "        source_node_idx = nodes_idx_dict[self.reachability_graph.starting_node.marking]\n",
    "        source_node = search_graph_nodes[source_node_idx]\n",
    "        source_node.dist = 0\n",
    "        \n",
    "        for node in search_graph_nodes:\n",
    "            heappush(distance_min_heap, node)\n",
    "        \n",
    "        while distance_min_heap:\n",
    "            min_dist_node = heappop(distance_min_heap)\n",
    "            need_heapify = False\n",
    "            \n",
    "            for neighbor_transition_tuple in min_dist_node.graph_node.neighbors:\n",
    "                neighbor, transition = neighbor_transition_tuple[0], neighbor_transition_tuple[1]\n",
    "                alt_distance = min_dist_node.dist + transition.weight\n",
    "                neighbor_search_idx = nodes_idx_dict[neighbor.marking]\n",
    "                    \n",
    "                if alt_distance < search_graph_nodes[neighbor_search_idx].dist:\n",
    "                    search_graph_nodes[neighbor_search_idx].dist = alt_distance\n",
    "                    search_graph_nodes[neighbor_search_idx].ancestor = min_dist_node\n",
    "                    search_graph_nodes[neighbor_search_idx].transition_to_ancestor = transition\n",
    "                    need_heapify = True\n",
    "            \n",
    "            if need_heapify:\n",
    "                heapify(distance_min_heap)\n",
    "        \n",
    "#         print('ending marking is: ', self.reachability_graph.ending_node.marking)\n",
    "#         print('nodes_idx_dict is: ', nodes_idx_dict)\n",
    "        final_mark_idx = nodes_idx_dict[self.reachability_graph.ending_node.marking]\n",
    "        curr_node = search_graph_nodes[final_mark_idx]\n",
    "        path = []\n",
    "\n",
    "        while curr_node is not source_node:\n",
    "#             path.append(curr_node.transition_to_ancestor.label)            \n",
    "            path.append(curr_node.transition_to_ancestor.name)\n",
    "            curr_node = curr_node.ancestor\n",
    "        \n",
    "        return path[::-1], search_graph_nodes[final_mark_idx].dist\n",
    "\n",
    "    \n",
    "    \n",
    "    def _dijkstra_no_rg_construct(self, prob_dict=None, lamda=0.5, return_final_marking=False):\n",
    "        distance_min_heap = []\n",
    "        heapify(distance_min_heap) \n",
    "        curr_node = search_node_new(self.init_mark, dist=0)\n",
    "        heappush(distance_min_heap, curr_node)        \n",
    "        marking_distance_dict = {}\n",
    "        visited_markings = set()\n",
    "        \n",
    "        if prob_dict is None:\n",
    "            prob_dict = {}\n",
    "            \n",
    "        while distance_min_heap:\n",
    "            min_dist_node = heappop(distance_min_heap)\n",
    "            \n",
    "            if min_dist_node.marking.places in visited_markings:\n",
    "                continue\n",
    "                \n",
    "            if min_dist_node.marking.places == self.final_mark.places:\n",
    "                break\n",
    "              \n",
    "            available_transitions = self._find_available_transitions(min_dist_node.marking.places)\n",
    "            for transition in available_transitions:\n",
    "                new_marking = self._fire_transition(min_dist_node.marking, transition)\n",
    "                \n",
    "                if new_marking.places in visited_markings:\n",
    "                    continue\n",
    "                                        \n",
    "                if new_marking in visited_markings:\n",
    "                    continue\n",
    "                    \n",
    "                conditioned_transition_weight = self.compute_conditioned_weight(min_dist_node.path_prefix, transition, prob_dict, lamda=lamda)\n",
    "                if new_marking.places not in marking_distance_dict or marking_distance_dict[new_marking.places] > min_dist_node.dist                                                                                                                                      + conditioned_transition_weight: \n",
    "                    new_path_prefix = min_dist_node.path_prefix + transition.label if transition.label is not None else min_dist_node.path_prefix\n",
    "                    \n",
    "                    new_node = search_node_new(new_marking,\n",
    "                                               dist=min_dist_node.dist+conditioned_transition_weight,\n",
    "                                               ancestor=min_dist_node,\n",
    "                                               transition_to_ancestor=transition,\n",
    "                                               path_prefix=new_path_prefix)\n",
    "                    \n",
    "                    marking_distance_dict[new_marking.places] = new_node.dist\n",
    "                    heappush(distance_min_heap, new_node)\n",
    "            \n",
    "            visited_markings.add(min_dist_node.marking.places)\n",
    "            \n",
    "                \n",
    "        shortest_path = []    \n",
    "        curr_node = min_dist_node\n",
    "        while curr_node.ancestor:\n",
    "            shortest_path.append(curr_node.transition_to_ancestor.name)  \n",
    "            curr_node = curr_node.ancestor\n",
    "        \n",
    "#         print('min_dist_node distance: ', min_dist_node.dist)\n",
    "#         print('shortest path: ', shortest_path[::-1])\n",
    "        if return_final_marking: #TO DO: need to include overlap in the code \n",
    "            return shortest_path[::-1], min_dist_node.dist, self.marking.place\n",
    "        \n",
    "        return shortest_path[::-1], min_dist_node.dist    \n",
    "                    \n",
    "\n",
    "        \n",
    "    def __fire_transition(self, mark, transition):\n",
    "        '''Input: Marking object, Transition object\n",
    "        Output: Marking object''' \n",
    "        \n",
    "        subtract_mark = [0] * len(mark.places)\n",
    "        for arc in transition.in_arcs:\n",
    "            place_idx = self.places_indices[arc.source.name]\n",
    "            subtract_mark[place_idx] -= arc.weight\n",
    "        \n",
    "        add_mark = [0] * len(mark.places)\n",
    "        for arc in transition.out_arcs:\n",
    "            place_idx = self.places_indices[arc.target.name]\n",
    "            add_mark[place_idx] += arc.weight\n",
    "  \n",
    "        new_mark = tuple([sum(x) for x in zip(list(mark.places), subtract_mark, add_mark)])\n",
    "        for elem in new_mark:\n",
    "            if elem < 0:\n",
    "                print(f'the mark was: {mark} and I subtract the following values: {subtract_mark} and adding these: {add_mark} \\\n",
    "                which results in this: {new_mark} and all this sh!t was by using this transition: {transition.name}')\n",
    "        new_mark_obj = Marking(new_mark)\n",
    "        \n",
    "        return new_mark_obj\n",
    "    \n",
    "    \n",
    "    def __update_sync_product_trans_names(self, transitions_list):\n",
    "\n",
    "        for trans in transitions_list:\n",
    "            if trans.move_type == 'model':\n",
    "                trans.name = f'({trans.name}, >>)'\n",
    "            elif trans.move_type == 'trace':\n",
    "                trans.name = f'(>>, {trans.name})'\n",
    "            else:\n",
    "                trans.name = f'({trans.name}, {trans.name})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-30T15:53:14.546774800Z",
     "start_time": "2024-08-30T15:53:14.532775900Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_probs(n_elements):\n",
    "    probs = []\n",
    "    for i in range(n_elements):\n",
    "        res = random.sample(range(1, 10), 3) \n",
    "        total = sum(res)\n",
    "        norm_res = [round(item / total, 2) for item in res]  \n",
    "        probs.append(norm_res)\n",
    "    return probs\n",
    "\n",
    "\n",
    "def smooth_zero_probs(probabilities_lst):\n",
    "    zero_indices = []\n",
    "    non_zero_donating_indices = []\n",
    "    for index, value in enumerate(probabilities_lst):\n",
    "        if value == 0:\n",
    "            zero_indices.append(index)\n",
    "            \n",
    "        elif value > 0.1:  \n",
    "            non_zero_donating_indices.append(index)\n",
    "    \n",
    "    assert non_zero_donating_indices == []\n",
    "    \n",
    "    for zero_index in zero_indices:\n",
    "        donating_index = random.choice(non_zero_donating_indices)\n",
    "        assert probabilities_lst[donating_index] <= 0.01        \n",
    "        probabilities_lst[zero_index] += 0.01\n",
    "        probabilities_lst[donating_index] -= 0.01\n",
    "        \n",
    "    return probabilities_lst\n",
    "\n",
    "\n",
    "def generate_argmax_probabilities(argmax_prob_is_right, n_activities):\n",
    "    '''\n",
    "    Given probability for argmax being the true activity the function generates probability list\n",
    "    '''\n",
    "    \n",
    "    new_probs_list = []\n",
    "    for i in range(n_activities):\n",
    "        new_probs_list.append(random.uniform(1.0, 10.0))\n",
    "    \n",
    "    max_value = max(new_probs_list)\n",
    "    max_value_idx = new_probs_list.index(max_value)\n",
    "    random_uniform_threshold = random.uniform(0,1)\n",
    "    \n",
    "    if argmax_prob_is_right > random_uniform_threshold:\n",
    "        if max_value_idx != 0:\n",
    "            new_probs_list[0], new_probs_list[max_value_idx] = new_probs_list[max_value_idx], new_probs_list[0]\n",
    "            \n",
    "    else:\n",
    "        if max_value_idx == 0:                      \n",
    "            random_idx_in_lst = random.choice([i for i in range(1, n_activities)])\n",
    "            counter = 0\n",
    "            while new_probs_list[random_idx_in_lst] == max_value:\n",
    "                random_idx_in_lst = random.choice([i for i in range(1, n_activities)])\n",
    "                counter += 1\n",
    "                if counter > 10:\n",
    "                    break\n",
    "                    \n",
    "            new_probs_list[0], new_probs_list[random_idx_in_lst] = new_probs_list[random_idx_in_lst], new_probs_list[0] \n",
    "    \n",
    "    normalize_factor = sum(new_probs_list)\n",
    "    new_probs_list_normalized = [round(value / normalize_factor, 2) for value in new_probs_list]\n",
    "    \n",
    "    return new_probs_list_normalized\n",
    "\n",
    "\n",
    "def generate_argmax_probabilities_unique(argmax_prob_is_right, n_activities):\n",
    "    \n",
    "    while True:\n",
    "        new_probs = generate_argmax_probabilities(argmax_prob_is_right, n_activities)\n",
    "        if len(new_probs) == len(set(new_probs)):\n",
    "            break\n",
    "    \n",
    "    return new_probs\n",
    "    \n",
    "    \n",
    "def generate_probabilities(true_value_prob, n_activities):\n",
    "    \n",
    "    new_probs_list = [true_value_prob]\n",
    "    noisy_probs = []\n",
    "    for i in range(n_activities - 1):\n",
    "        noisy_probs.append(random.uniform(1.0, 10.0))\n",
    "\n",
    "    normalize_factor = sum(noisy_probs) / (1-true_value_prob)\n",
    "\n",
    "    noisy_probs_normalized = [item / normalize_factor for item in noisy_probs]\n",
    "\n",
    "    new_probs_list += noisy_probs_normalized\n",
    "    new_probs_list = [round(item,2) for item in new_probs_list]\n",
    "    \n",
    "    return new_probs_list\n",
    "\n",
    "\n",
    "def convert_to_integer(num):\n",
    "    if isinstance(num, (np.ndarray, np.generic)):\n",
    "        return num.item()\n",
    "    \n",
    "    return int(num)\n",
    "\n",
    "def generate_new_record(trace_df, idx):\n",
    "    new_dict = {}\n",
    "    new_dict['concept:name'] = [[activity for activity in trace_df.iloc[idx]['concept:name']]]\n",
    "    new_dict['case:concept:name'] = trace_df.iloc[idx]['case:concept:name']\n",
    "    new_dict['probs'] = [[prob for prob in trace_df.iloc[idx]['probs']]]\n",
    "    new_record = pd.DataFrame(new_dict)\n",
    "    \n",
    "    return new_record\n",
    "\n",
    "\n",
    "def copy_trace(trace, for_determ=True):\n",
    "    \n",
    "    new_trace_df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(trace)):\n",
    "        new_trace_df = pd.concat([new_trace_df, generate_new_record(trace, i)])\n",
    "    \n",
    "    new_trace_df = new_trace_df.reset_index(drop=True)  \n",
    "    \n",
    "    if for_determ:\n",
    "        new_trace_df['concept:name'] = new_trace_df['concept:name'].apply(lambda x: x[0]) \n",
    "        \n",
    "    return new_trace_df\n",
    "\n",
    "\n",
    "def copy_dataframe(df):\n",
    "    \n",
    "    traces_df_list = []\n",
    "    traces_ids = df['case:concept:name'].unique()\n",
    "    \n",
    "    for trace_id in traces_ids:\n",
    "        trace_df = df[df['case:concept:name'] == trace_id]\n",
    "        trace_df_copy = copy_trace(trace_df, for_determ=False)\n",
    "        traces_df_list.append(trace_df_copy)\n",
    "    \n",
    "    new_df_copy = pd.concat(traces_df_list, ignore_index=True)\n",
    "    \n",
    "    return new_df_copy\n",
    "\n",
    "\n",
    "# def alter_activity_labels(trace_df, activities_universe, alter_fraq=0.5):\n",
    "    \n",
    "#     for idx in range(len(trace_df)):\n",
    "#         if random.uniform(0,1) < alter_fraq:\n",
    "#             curr_activity = {trace_df.at[idx, 'concept:name'][0]}\n",
    "#             new_activity = random.choice(list(activities_universe.copy().difference(curr_activity))) \n",
    "#             trace_df.at[idx,'concept:name'] = [new_activity]\n",
    "\n",
    "#     return trace_df\n",
    "\n",
    "\n",
    "def alter_activity_labels(trace_df, activities_universe, alter_fraq):\n",
    "    \n",
    "    n_activities_to_alter = int(round(len(trace_df) * alter_fraq))\n",
    "    \n",
    "    if n_activities_to_alter == 0:\n",
    "        return trace_df\n",
    "    \n",
    "    rows_expansion_indexes = random.sample(range(len(trace_df)), n_activities_to_alter)        \n",
    "\n",
    "    for idx in rows_expansion_indexes: \n",
    "        curr_activity = {trace_df.at[idx, 'concept:name'][0]}\n",
    "        new_activity = random.choice(list(activities_universe.copy().difference(curr_activity))) \n",
    "        trace_df.at[idx,'concept:name'] = [new_activity]\n",
    "\n",
    "    return trace_df\n",
    "\n",
    "\n",
    "def swap_successor_events(trace_df, swap_fraq=0.5):\n",
    "    \n",
    "    indices = [i for i in range(len(trace_df))]\n",
    "    indices_to_swap = [idx for idx in indices if random.uniform(0,1) < swap_fraq]\n",
    "    left_right_swap = [int(round(random.uniform(0,1))) for _ in indices_to_swap]\n",
    "    \n",
    "    if len(indices_to_swap) == 0:\n",
    "        return trace_df\n",
    "    \n",
    "    if indices_to_swap[0] == 0:\n",
    "        left_right_swap[0] = 1\n",
    "    \n",
    "    if indices_to_swap[-1] == len(trace_df) - 1:\n",
    "        left_right_swap[-1] = 0\n",
    "    \n",
    "    indices_to_swap = set(indices_to_swap)\n",
    "    \n",
    "    left_right_idx = 0\n",
    "    for i, index in enumerate(indices):\n",
    "        if index in indices_to_swap:\n",
    "            if left_right_swap[left_right_idx] == 0:\n",
    "                indices[i], indices[i-1] = indices[i-1], indices[i]\n",
    "                \n",
    "            elif i+1 < len(trace_df):\n",
    "                indices[i], indices[i+1] = indices[i+1], indices[i]\n",
    "            \n",
    "            indices_to_swap.remove(index)\n",
    "            left_right_idx += 1\n",
    "            \n",
    "    trace_df = trace_df.reindex(indices).reset_index(drop=True)\n",
    "    \n",
    "    return trace_df\n",
    "\n",
    "\n",
    "def duplicate_activities(trace_df, duplicate_fraq=0.5):\n",
    "#     indices_to_duplicate = [i for i in range(len(trace_df)) if random.uniform(0,1) < duplicate_fraq]\n",
    "\n",
    "    n_indices_to_duplicate = int(round(len(trace_df) * duplicate_fraq))\n",
    "    indices_to_duplicate = [i for i in range(len(trace_df))]\n",
    "    indices_to_duplicate = sorted(random.sample(indices_to_duplicate, n_indices_to_duplicate))\n",
    "    \n",
    "#     indices_to_duplicate_doesnt_work = list(random.sample([i for i in range(len(trace_df))], n_indices_to_duplicate))\n",
    "#     print(f'good indices:{indices_to_duplicate} and bad indices:{indices_to_duplicate_doesnt_work}')\n",
    "#     if n_indices_to_duplicate == 0:\n",
    "#         return trace_df\n",
    "    \n",
    "#     if len(indices_to_duplicate) == 0:\n",
    "#         return trace_df    \n",
    "    \n",
    "#     indices_to_duplicate = random.sample(range(len(trace_df)), n_indices_to_duplicate)\n",
    "\n",
    "    curr_idx = 0\n",
    "    new_duplicated_df = pd.DataFrame(columns = trace_df.columns)\n",
    "    \n",
    "    if not indices_to_duplicate:\n",
    "        return trace_df\n",
    "    \n",
    "    for idx in indices_to_duplicate:\n",
    "        new_duplicated_df = pd.concat([new_duplicated_df, trace_df.iloc[curr_idx:idx], generate_new_record(trace_df, idx)])\n",
    "        curr_idx = idx\n",
    "        new_duplicated_df = new_duplicated_df.reset_index(drop=True)\n",
    "        \n",
    "    new_duplicated_df = pd.concat([new_duplicated_df, trace_df.iloc[idx:].copy()])\n",
    "    new_duplicated_df = new_duplicated_df.reset_index(drop=True)\n",
    "    \n",
    "    return new_duplicated_df\n",
    "\n",
    "########original##############\n",
    "# def add_noise_by_trace(df, disturbed_trans_frac=0.3, true_trans_prob=0.5, expansion_min=2, expansion_max=4, \\\n",
    "#                         randomized_true_trans_prob = False, generate_probs_for_argmax=False):\n",
    "    \n",
    "#     all_activities_unique = set([activity[0] for activity in df['concept:name'].tolist()])\n",
    "\n",
    "#     unique_cases_ids = df['case:concept:name'].unique()\n",
    "\n",
    "#     first_case_df = True\n",
    "#     for case_id in unique_cases_ids:\n",
    "#         case_df = df[df['case:concept:name'] == case_id]\n",
    "#         case_df = case_df.reset_index(drop = True)\n",
    "#         case_df = add_noise(case_df, disturbed_trans_frac=disturbed_trans_frac, true_trans_prob=true_trans_prob,\n",
    "#                             expansion_min=expansion_min, expansion_max=expansion_max,\n",
    "#                             randomized_true_trans_prob=randomized_true_trans_prob, generate_probs_for_argmax=generate_probs_for_argmax,\n",
    "#                             all_activities_unique=all_activities_unique)\n",
    "\n",
    "#         if not first_case_df:\n",
    "#             df_new = df_new.append(case_df, ignore_index = True)\n",
    "        \n",
    "#         else:\n",
    "#             df_new = case_df\n",
    "#             first_case_df = False\n",
    "            \n",
    "#     return df_new\n",
    "\n",
    "def add_noise_by_trace(df, disturbed_trans_frac=0.3, true_trans_prob=0.5, expansion_min=2, expansion_max=4, \\\n",
    "                       randomized_true_trans_prob = False, generate_probs_for_argmax=False, gradual_noise=True,\n",
    "                       alter_labels=False, swap_events=False, duplicate_acts=False, fraq=0.1, determ_noise_only=False):\n",
    "    \n",
    "    all_activities_unique = set([activity[0] for activity in df['concept:name'].tolist()])\n",
    "    unique_cases_ids = df['case:concept:name'].unique()\n",
    "    \n",
    "    new_noised_deterministic_df = pd.DataFrame(columns = df.columns)\n",
    "    first_case_df = True\n",
    "    for case_id in unique_cases_ids:\n",
    "        case_df = df[df['case:concept:name'] == case_id]\n",
    "        case_df = case_df.reset_index(drop = True)\n",
    "\n",
    "        if alter_labels:\n",
    "            case_df = alter_activity_labels(case_df, all_activities_unique, fraq)\n",
    "\n",
    "        if swap_events:         \n",
    "            case_df = swap_successor_events(case_df, fraq)\n",
    " \n",
    "        if duplicate_acts:            \n",
    "            case_df = duplicate_activities(case_df, fraq)\n",
    "\n",
    "#         new_noised_deterministic_df = pd.concat([new_noised_deterministic_df, copy_trace(case_df)])\n",
    "        if determ_noise_only is False:\n",
    "        \n",
    "            if gradual_noise:\n",
    "                case_df = add_gradual_noise(case_df, disturbed_trans_frac=disturbed_trans_frac, true_trans_prob=true_trans_prob,\n",
    "                                    expansion_min=expansion_min, expansion_max=expansion_max,\n",
    "                                    randomized_true_trans_prob=randomized_true_trans_prob, generate_probs_for_argmax=generate_probs_for_argmax,\n",
    "                                    all_activities_unique=all_activities_unique)\n",
    "\n",
    "            else:\n",
    "                case_df = add_noise(case_df, disturbed_trans_frac=disturbed_trans_frac, true_trans_prob=true_trans_prob,\n",
    "                                    expansion_min=expansion_min, expansion_max=expansion_max,\n",
    "                                    randomized_true_trans_prob=randomized_true_trans_prob, generate_probs_for_argmax=generate_probs_for_argmax,\n",
    "                                    all_activities_unique=all_activities_unique)\n",
    "            \n",
    "\n",
    "        if not first_case_df:\n",
    "            df_new = df_new.append(case_df, ignore_index = True)\n",
    "        \n",
    "        else:\n",
    "            df_new = case_df\n",
    "            first_case_df = False\n",
    "    \n",
    "#     new_noised_deterministic_df = new_noised_deterministic_df.reset_index(drop=True)\n",
    "    \n",
    "    return df_new #, new_noised_deterministic_df\n",
    "\n",
    "\n",
    "def add_noise(df, disturbed_trans_frac=0.3, true_trans_prob=0.5, expansion_min=2, expansion_max=4, \\\n",
    "                              randomized_true_trans_prob = False, generate_probs_for_argmax=False, all_activities_unique=None):\n",
    "    # print(f'disturbed_trans_frac is: {disturbed_trans_frac} and df len is: {len(df)} and their multiply is: {disturbed_trans_frac * len(df)} and rounding this number results in:{round(disturbed_trans_frac * len(df))}')\n",
    "    n_rows = int(round(disturbed_trans_frac * len(df)))\n",
    "    # print(f'df len is: {len(df)} and n_rows for parallel activities are: {n_rows}')\n",
    "    rows_expansion_indexes = random.sample(range(len(df)), n_rows)\n",
    "    all_activities_unique = set([activity[0] for activity in df['concept:name'].tolist()]) if all_activities_unique is None else all_activities_unique \n",
    "\n",
    "    for idx in rows_expansion_indexes:\n",
    "        if randomized_true_trans_prob:\n",
    "            true_trans_prob = np.random.uniform(0.01,0.99) \n",
    "        curr_activity = {df.at[idx, 'concept:name'][0]}\n",
    "\n",
    "        n_expanded_trans = random.randint(expansion_min, expansion_max)\n",
    "        assert n_expanded_trans < len(all_activities_unique) - 1\n",
    "        noisy_activities = random.sample(all_activities_unique.copy().difference(curr_activity), n_expanded_trans - 1) \n",
    "\n",
    "        noisy_activities = [activity for activity in noisy_activities]\n",
    "        \n",
    "        for activity in noisy_activities:\n",
    "            df.at[idx,'concept:name'].append(activity)\n",
    "            \n",
    "        if generate_probs_for_argmax:\n",
    "            df.at[idx,'probs'] = generate_argmax_probabilities_unique(true_trans_prob, n_expanded_trans)\n",
    "\n",
    "        else:\n",
    "            df.at[idx,'probs'] = generate_probabilities(true_trans_prob, n_expanded_trans)\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_gradual_noise(df, disturbed_trans_frac=0.3, true_trans_prob=0.5, expansion_min=2, expansion_max=4, \\\n",
    "                        randomized_true_trans_prob = False, generate_probs_for_argmax=False, all_activities_unique=None):\n",
    "\n",
    "    deterministic_idxs = []\n",
    "    total_disturbed_transitions = 0\n",
    "    for idx, transition in enumerate(df['concept:name']):\n",
    "        if len(transition) > 1:\n",
    "            total_disturbed_transitions += 1\n",
    "        else:\n",
    "            deterministic_idxs.append(idx)\n",
    "    \n",
    "    current_disturbed_transitions_fraq = total_disturbed_transitions / len(df)\n",
    "    \n",
    "    if current_disturbed_transitions_fraq >= disturbed_trans_frac:\n",
    "        return df\n",
    "    \n",
    "    n_additional_trans_to_disturb = int(round((disturbed_trans_frac - current_disturbed_transitions_fraq) * len(df)))\n",
    "    \n",
    "    if n_additional_trans_to_disturb == 0:\n",
    "        return df\n",
    "    \n",
    "    rows_expansion_indexes = random.sample(deterministic_idxs, n_additional_trans_to_disturb)\n",
    "\n",
    "    all_activities_unique = set([activity for activity_lst in df['concept:name'].tolist() for activity in activity_lst]) if all_activities_unique is None else all_activities_unique \n",
    "\n",
    "    for idx in rows_expansion_indexes:\n",
    "        if randomized_true_trans_prob:\n",
    "            true_trans_prob = np.random.uniform(0.01,0.99) \n",
    "        curr_activity = {df.at[idx, 'concept:name'][0]}\n",
    "\n",
    "        n_expanded_trans = random.randint(expansion_min, expansion_max)\n",
    "\n",
    "        assert n_expanded_trans < len(all_activities_unique) - 1\n",
    "        noisy_activities = random.sample(list(all_activities_unique.copy().difference(curr_activity)), n_expanded_trans - 1) \n",
    "\n",
    "        noisy_activities = [activity for activity in noisy_activities]\n",
    "\n",
    "        for activity in noisy_activities:\n",
    "            df.at[idx, 'concept:name'].append(activity)\n",
    "            \n",
    "        if generate_probs_for_argmax:\n",
    "            df.at[idx, 'probs'] = generate_argmax_probabilities_unique(true_trans_prob, n_expanded_trans)\n",
    "\n",
    "        else:\n",
    "            df.at[idx,'probs'] = generate_probabilities(true_trans_prob, n_expanded_trans)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_all_probs_one(preprocessed_test_data):\n",
    "    \n",
    "    df_one_probs = preprocessed_test_data.copy(deep=True)\n",
    "    df_one_probs['probs'] = df_one_probs['probs'].apply(lambda x: [1 for prob in x])\n",
    "    \n",
    "    return df_one_probs \n",
    "\n",
    "    \n",
    "def construct_trace_model(trace_df, non_sync_move_penalty=1):\n",
    "\n",
    "    places = [Place(f'place_{i}') for i in range(len(trace_df) + 1)]\n",
    "    transitions = []\n",
    "    transition_to_idx_dict = {}\n",
    "    curr_idx = 0\n",
    "    \n",
    "    for i in range(len(trace_df)):\n",
    "        for idx, activity in enumerate(trace_df.iloc[i,0]):\n",
    "            new_transition = Transition(f'{activity}_{i+1}', activity, prob=trace_df.iloc[i,1][idx], weight=non_sync_move_penalty)\n",
    "            transitions.append(new_transition)\n",
    "            transition_to_idx_dict[f'{activity}_{i+1}'] = curr_idx\n",
    "            new_transition.location_idx = i\n",
    "            curr_idx += 1\n",
    "\n",
    "    trace_model_net = PetriNet('trace_model', places, transitions, trace_len=len(trace_df))\n",
    "\n",
    "    for i in range(len(trace_df)):\n",
    "        for activity in trace_df.iloc[i,0]:       \n",
    "            trace_model_net.add_arc_from_to(places[i], transitions[transition_to_idx_dict[f'{activity}_{i+1}']])\n",
    "            trace_model_net.add_arc_from_to(transitions[transition_to_idx_dict[f'{activity}_{i+1}']], places[i+1])\n",
    "    \n",
    "    init_mark = tuple([1] + [0] * len(trace_df))\n",
    "    final_mark = tuple([0] * len(trace_df) + [1])\n",
    "    \n",
    "    trace_model_net.init_mark = Marking(init_mark)\n",
    "    trace_model_net.final_mark = Marking(final_mark)\n",
    "    \n",
    "    return trace_model_net\n",
    "            \n",
    "    \n",
    "def pre_process_log(df_log):\n",
    "    df_log = copy.deepcopy(df_log)\n",
    "    clean_df_log = df_log[['concept:name', 'case:concept:name']]\n",
    "    clean_df_log['probs'] = [[1.0]] * len(clean_df_log)\n",
    "    clean_df_log['probs'].astype('object')\n",
    "    clean_df_log['concept:name'] = clean_df_log['concept:name'].apply(lambda activity: [activity])\n",
    "    \n",
    "    return clean_df_log\n",
    "\n",
    "\n",
    "def calc_conf_for_log(df_log, model, non_sync_move_penalty=1, add_heuristic=False):\n",
    "    unique_cases_ids = df_log['case:concept:name'].unique()\n",
    "    conformance_scores_lst = []\n",
    "    \n",
    "    for case_id in unique_cases_ids:\n",
    "        case_df = df_log[df_log['case:concept:name'] == case_id]\n",
    "        case_df = case_df[['concept:name', 'probs']]\n",
    "        case_df = case_df.reset_index(drop = True)\n",
    "        case_trace_model = construct_trace_model(case_df, non_sync_move_penalty, add_heuristic=add_heuristic)\n",
    "        case_conformance_score = model.conformance_checking(case_trace_model)[1]\n",
    "        conformance_scores_lst.append(case_conformance_score)\n",
    "    \n",
    "    return mean(conformance_scores_lst)\n",
    "\n",
    "        \n",
    "def sort_places(places):\n",
    "    init_mark = [place for place in places if place.name == 'source']\n",
    "    final_mark = [place for place in places if place.name == 'sink']\n",
    "    inner_places = [place for place in places if place.name not in {'source', 'sink'}]\n",
    "    inner_places_sorted = sorted(inner_places, key=lambda x: float(x.name[2:]))\n",
    "    places_sorted = init_mark + inner_places_sorted + final_mark\n",
    "    \n",
    "    return places_sorted\n",
    "\n",
    "\n",
    "def from_discovered_model_to_PetriNet(discovered_model, non_sync_move_penalty=1, name='discovered_net'):\n",
    "    \n",
    "    discovered_model = copy.deepcopy(discovered_model)\n",
    "    places = sort_places(discovered_model.places) \n",
    "    places = [Place(p.name) for p in places]\n",
    "    \n",
    "    petri_new_arcs = []\n",
    "    transition_list = list(discovered_model.transitions)\n",
    "    \n",
    "    assert len([tran.name for tran in transition_list]) == len(set([tran.name for tran in transition_list]))\n",
    "    assert len([place.name for place in places]) == len(set([place.name for place in places]))\n",
    "    \n",
    "    tran2idx = {tran.name: i for i, tran in enumerate(transition_list)}\n",
    "    place2idx = {place.name: i for i, place in enumerate(places)}\n",
    "    \n",
    "    transitions = [Transition(transition.name, transition.label, transition.in_arcs, transition.out_arcs, 'model', weight = non_sync_move_penalty) \\\n",
    "                                                                                for transition in transition_list]\n",
    "    for trans in transitions:\n",
    "        if trans.label is None:\n",
    "            trans.weight = 0\n",
    "            \n",
    "    \n",
    "    for i in range(len(transitions)):\n",
    "        new_in_arcs = set()\n",
    "        for arc in transitions[i].in_arcs:\n",
    "            new_in_arc = Arc(places[place2idx[arc.source.name]], transitions[i])\n",
    "            new_in_arcs.add(new_in_arc)\n",
    "            petri_new_arcs.append(new_in_arc)\n",
    "            \n",
    "        new_out_arcs = set()   \n",
    "        for arc in transitions[i].out_arcs:\n",
    "            new_out_arc = Arc(transitions[i], places[place2idx[arc.target.name]])\n",
    "            new_out_arcs.add(new_out_arc)\n",
    "            petri_new_arcs.append(new_out_arc)\n",
    "        \n",
    "        transitions[i].in_arcs = new_in_arcs\n",
    "        transitions[i].out_arcs = new_out_arcs\n",
    "    \n",
    "    for transition in transitions:\n",
    "        if transition.label is not None:\n",
    "            transition.name = transition.label\n",
    "\n",
    "        \n",
    "    init_mark = tuple([1] + [0] * (len(places) - 1))\n",
    "    final_mark = tuple([0] * (len(places) - 1) + [1])\n",
    "    \n",
    "    new_PetriNet = PetriNet(name)\n",
    "    new_PetriNet.add_places(places)\n",
    "    new_PetriNet.add_transitions(transitions)\n",
    "    new_PetriNet.init_mark = Marking(init_mark)\n",
    "    new_PetriNet.final_mark = Marking(final_mark)\n",
    "    new_PetriNet.arcs = petri_new_arcs\n",
    "        \n",
    "    return new_PetriNet\n",
    "\n",
    "\n",
    "# def argmax_stochastic_trace(stochastic_trace_df):\n",
    "#     determ_df = pd.DataFrame(columns=['concept:name', 'case:concept:name', 'probs'])\n",
    "    \n",
    "#     for i in range(len(stochastic_trace_df)):\n",
    "#         max_val = max(stochastic_trace_df.iloc[i,2])\n",
    "#         max_idx = stochastic_trace_df.iloc[i,2].index(max_val)\n",
    "#         highest_prob_activity = stochastic_trace_df.iloc[i,0][max_idx]\n",
    "#         case_id =  stochastic_trace_df.iloc[i,1]\n",
    "#         determ_df = determ_df.append({'concept:name': highest_prob_activity, 'case:concept:name': case_id, 'probs':[1.0]}, ignore_index=True)\n",
    "        \n",
    "#     return determ_df\n",
    "\n",
    "\n",
    "def argmax_stochastic_trace(stochastic_trace_df):\n",
    "    # Initialize an empty list to hold dictionaries\n",
    "    data_list = []\n",
    "    \n",
    "    for i in range(len(stochastic_trace_df)):\n",
    "        # Assuming stochastic_trace_df.iloc[i, 2] is a list or similar iterable with probabilities\n",
    "        max_val = max(stochastic_trace_df.iloc[i, 2])\n",
    "        max_idx = stochastic_trace_df.iloc[i, 2].index(max_val)\n",
    "        highest_prob_activity = stochastic_trace_df.iloc[i, 0][max_idx]\n",
    "        case_id = stochastic_trace_df.iloc[i, 1]\n",
    "        \n",
    "        # Append a new dictionary for each row to be added to the DataFrame\n",
    "        data_list.append({'concept:name': highest_prob_activity, 'case:concept:name': case_id, 'probs': [1.0]})\n",
    "    \n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    determ_df = pd.DataFrame(data_list)\n",
    "    \n",
    "    return determ_df\n",
    "\n",
    "\n",
    "def get_non_sync_non_quiet_activities(alignment, quiet_activities):\n",
    "    non_sync_non_quiet_transitions = []\n",
    "    for trans in alignment:\n",
    "        if '>>' in trans:\n",
    "            if not any(activity for activity in quiet_activities if activity in trans):\n",
    "                non_sync_non_quiet_transitions.append(trans) \n",
    "\n",
    "    \n",
    "    return non_sync_non_quiet_transitions    \n",
    "\n",
    "\n",
    "def get_sync_activities(alignment):\n",
    "    return [align for align in alignment if '>>' not in align]      \n",
    "\n",
    "\n",
    "def alignment_accuracy_helper(alignment, true_trace_df):\n",
    "    stochastic_align_clean = [item.split(',')[1][1:-1] for item in alignment[0] if item.split(',')[1][1:-1] != '>>']\n",
    "    determ_trace_clean = true_trace_df['concept:name'].tolist()\n",
    "#     print('true trace df is: ', true_trace_df)\n",
    "#     print('determ trace clean is: ', determ_trace_clean)\n",
    "    similar_activity = 0\n",
    "    for idx, item in enumerate(determ_trace_clean):\n",
    "        if item in stochastic_align_clean[idx]:\n",
    "            similar_activity += 1\n",
    "        \n",
    "    return similar_activity / len(determ_trace_clean), len(determ_trace_clean)\n",
    "\n",
    "\n",
    "def compare_argmax_and_stochastic_alignments(stochastic_trace_df, true_trace_df, model, non_sync_penalty=1, add_heuristic=False):\n",
    "    df_stochastic = stochastic_trace_df[['concept:name', 'probs']]\n",
    "    df_stochastic = df_stochastic.reset_index(drop = True)\n",
    "    case_trace_stochastic_model = construct_trace_model(df_stochastic, non_sync_penalty)\n",
    "\n",
    "#     case_trace_stochastic_model = construct_trace_model(df_stochastic, non_sync_penalty, add_heuristic=add_heuristic)\n",
    "    stochastic_alignment = model.conformance_checking(case_trace_stochastic_model)\n",
    "#     print('True trace is:')\n",
    "#     display(true_trace_df)\n",
    "#     print()\n",
    "#     print('The modified trace is:')\n",
    "#     display(df_stochastic)\n",
    "#     print()\n",
    "#     print(f'The alignment is: {stochastic_alignment[0]}')\n",
    "#     print(f'The cost of the alignment is: {stochastic_alignment[1]}')\n",
    "#     print()\n",
    "    argmax_trace = argmax_stochastic_trace(stochastic_trace_df)\n",
    "#     argmax_trace_preprocessed = pre_process_log(argmax_trace)\n",
    "#     df_deterministic = argmax_trace_preprocessed[['concept:name', 'probs']]\n",
    "#     df_deterministic = df_deterministic.reset_index(drop = True)\n",
    "#     case_trace_deterministic_model = construct_trace_model(df_deterministic, non_sync_penalty, add_heuristic=add_heuristic)    \n",
    "#     argmax_alignment = model.conformance_checking(case_trace_deterministic_model)\n",
    "\n",
    "#     quiet_activities = {transition.name for transition in model.transitions if transition.label is None}\n",
    "#     non_sync_non_quiet_stochastic_alignment_activities = get_non_sync_non_quiet_activities(stochastic_alignment[0], quiet_activities)\n",
    "#     sync_stochastic_alignment_activities = get_sync_activities(stochastic_alignment[0])  \n",
    "#     non_sync_non_quiet_argmax_alignment_activities = get_non_sync_non_quiet_activities(argmax_alignment[0], quiet_activities)\n",
    "#     sync_argmax_alignment_activities = get_sync_activities(argmax_alignment[0])     \n",
    "    stochastic_acc, trace_len = alignment_accuracy_helper(stochastic_alignment, true_trace_df)\n",
    "    argmax_acc = sum(argmax_trace['concept:name'].reset_index(drop=True) == true_trace_df['concept:name'].reset_index(drop=True)) / len(argmax_trace)\n",
    "\n",
    "#     return non_sync_non_quiet_stochastic_alignment_activities, sync_stochastic_alignment_activities, \\\n",
    "#            non_sync_non_quiet_argmax_alignment_activities, sync_argmax_alignment_activities, \\\n",
    "#            stochastic_acc, argmax_acc, trace_len, stochastic_alignment[1]\n",
    "\n",
    "    return stochastic_acc, argmax_acc\n",
    "\n",
    "\n",
    "def generate_statistics_for_dataset(stochastic_dataset, df_test, model, shortest_path_in_model, non_sync_penalty=1, add_heuristic=False):\n",
    "    '''Generate 4 arrays for plots. \n",
    "       Input: stochastic dataframe with multiple traces\n",
    "       Output: 4 arrays with amounts of sync and non_syc activities'''\n",
    "\n",
    "    unique_cases_ids = stochastic_dataset['case:concept:name'].unique()\n",
    "    \n",
    "#     non_sync_non_quiet_stochastic_alignment_len = []\n",
    "#     sync_stochastic_alignment_len = []\n",
    "#     non_sync_non_quiet_argmax_len = []\n",
    "#     sync_argmax_alignment_len = []\n",
    "#     stochastic_acc_lst = []\n",
    "#     argmax_acc_lst = []\n",
    "#     traces_length = []\n",
    "    stochastic_conformance_scores_lst = []\n",
    "#     fitness_lst = []\n",
    "    \n",
    "    for case_id in unique_cases_ids:\n",
    "        case_df = stochastic_dataset[stochastic_dataset['case:concept:name'] == case_id]\n",
    "        case_df = case_df.reset_index(drop = True)\n",
    "        true_case_df = df_test[df_test['case:concept:name'] == case_id]\n",
    "#         worst_alignment_score = shortest_path_in_model + len(case_df)\n",
    "        \n",
    "#         non_sync_non_quiet_stochastic_alignment_activities, sync_stochastic_alignment_activities, \\\n",
    "#         non_sync_non_quiet_argmax_alignment_activities, sync_argmax_alignment_activities, stochastic_acc, \\\n",
    "#         argmax_acc, trace_len, stoc_conf_score = compare_argmax_and_stochastic_alignments(case_df, true_case_df, model, non_sync_penalty, add_heuristic=add_heuristic)                               \n",
    "#         fitness = 1 - stoc_conf_score / worst_alignment_score\n",
    "        stoc_conf_score = compare_argmax_and_stochastic_alignments(case_df, true_case_df, model, non_sync_penalty, add_heuristic=add_heuristic) \n",
    "#         non_sync_non_quiet_stochastic_alignment_len.append(len(non_sync_non_quiet_stochastic_alignment_activities))\n",
    "#         sync_stochastic_alignment_len.append(len(sync_stochastic_alignment_activities))\n",
    "#         non_sync_non_quiet_argmax_len.append(len(non_sync_non_quiet_argmax_alignment_activities))\n",
    "#         sync_argmax_alignment_len.append(len(sync_argmax_alignment_activities))\n",
    "#         stochastic_acc_lst.append(stochastic_acc) \n",
    "#         argmax_acc_lst.append(argmax_acc) \n",
    "#         traces_length.append(trace_len)\n",
    "        stochastic_conformance_scores_lst.append(stoc_conf_score)\n",
    "#         fitness_lst.append(fitness)\n",
    "        \n",
    "#     return non_sync_non_quiet_stochastic_alignment_len, sync_stochastic_alignment_len, \\\n",
    "#            non_sync_non_quiet_argmax_len, sync_argmax_alignment_len, stochastic_acc_lst, argmax_acc_lst, traces_length, \\\n",
    "#            stochastic_conformance_scores_lst, fitness_lst\n",
    "        \n",
    "    return stochastic_conformance_scores_lst\n",
    "\n",
    "\n",
    "def calculate_statistics_for_different_uncertainty_levels(df, non_sync_penalty=1, n_traces_for_model_building = 15,\n",
    "                                                          true_trans_prob=None, expansion_min=2, expansion_max=2,             \n",
    "                                                          uncertainty_levels=None, generate_probs_for_argmax=False, \n",
    "                                                          by_trace=True, add_heuristic=False, custom_traces_addition=False,\n",
    "                                                          deterministic_evaluation=False, return_shortest_path_only=False,\n",
    "                                                          all_probs_one=False, gradual_noise=True, eval_lower_bound=True,\n",
    "                                                          alter_labels=False, swap_events=False, duplicate_acts=False,\n",
    "                                                          determ_noise_fraq=0.3, determ_noised_dataset=None):\n",
    "    \n",
    "   \n",
    "    if uncertainty_levels is None:\n",
    "        uncertainty_levels = np.linspace(0,1,21)\n",
    "    \n",
    "    train_traces = list(df['case:concept:name'].unique())[:n_traces_for_model_building]\n",
    "    if custom_traces_addition:\n",
    "        additional_traces = [173823, 173793, 173754, 173739, 173736, 173778, 173763, 173847]\n",
    "        additional_traces = [np.int64(item) for item in additional_traces]\n",
    "        train_traces += additional_traces\n",
    "        train_traces = list(set(train_traces))\n",
    "        \n",
    "    train_data = df[df['case:concept:name'].isin(train_traces)]\n",
    "    test_data = df[~df['case:concept:name'].isin(train_traces)]\n",
    "    \n",
    "    net, initial_marking, final_marking = pm4py.discover_petri_net_inductive(train_data)\n",
    "    discovered_net = from_discovered_model_to_PetriNet(net, non_sync_move_penalty=non_sync_penalty) \n",
    "    shortest_path_in_model = discovered_net.astar()[1]\n",
    "    \n",
    "    if return_shortest_path_only:\n",
    "        train_traces_length = [len(train_data[train_data['case:concept:name'] == trace_id]) for trace_id in train_traces]\n",
    "        mean_trace_length = mean(train_traces_length)\n",
    "        return shortest_path_in_model, mean_trace_length\n",
    "    \n",
    "    if deterministic_evaluation:\n",
    "        test_data_copy = test_data.copy()\n",
    "        test_data_copy['probs'] = [[1.0]] * len(test_data_copy)\n",
    "        test_data_copy['concept:name'] = test_data_copy['concept:name'].apply(lambda x: [x])\n",
    "        return calc_conf_for_log(test_data_copy, discovered_net, non_sync_move_penalty=non_sync_penalty, add_heuristic=add_heuristic)\n",
    "        \n",
    "    if true_trans_prob is None:\n",
    "        true_trans_prob = np.linspace(0,1,21)\n",
    "    \n",
    "    if not isinstance(true_trans_prob, (np.ndarray, list)):\n",
    "        true_trans_prob = [true_trans_prob]\n",
    "    \n",
    "    \n",
    "#     non_sync_stochastic_avgs = []\n",
    "#     sync_stochastic_avgs = []\n",
    "#     non_sync_argmax_avgs = []\n",
    "#     sync_argmax_avgs = []\n",
    "#     stochastic_acc_avgs = []\n",
    "#     argmax_acc_avgs = []\n",
    "    stochastic_conformance_avgs = []\n",
    "#     fitness_avgs = []\n",
    "    lower_bound_scores = []\n",
    "    \n",
    "    original_alter_labels, original_swap_events, original_duplicate_acts = alter_labels, swap_events, duplicate_acts\n",
    "    \n",
    "    for true_prob in true_trans_prob:\n",
    "        true_prob = round(true_prob, 2)\n",
    "        \n",
    "        preprocessed_test_data=None\n",
    "        determ_preprocess_flag = 1\n",
    "        \n",
    "        for uncert_level in uncertainty_levels:        \n",
    "            uncert_level = round(uncert_level, 2)\n",
    "            \n",
    "            if determ_preprocess_flag == 1:\n",
    "                alter_labels, swap_events, duplicate_acts = original_alter_labels, original_swap_events, original_duplicate_acts\n",
    "            \n",
    "            else:\n",
    "                alter_labels, swap_events, duplicate_acts = False, False, False\n",
    "            \n",
    "            if preprocessed_test_data is None and determ_noised_dataset is not None:\n",
    "                preprocessed_test_data = determ_noised_dataset\n",
    "            \n",
    "            elif preprocessed_test_data is None or gradual_noise is False:\n",
    "                preprocessed_test_data = pre_process_log(test_data.copy(deep=True).reset_index(drop=True))\n",
    "                \n",
    "            else:\n",
    "                preprocessed_test_data = copy_dataframe(preprocessed_test_data_noised)\n",
    "                \n",
    "                \n",
    "            if by_trace is True:\n",
    "#                 print('dataset before noise:')\n",
    "#                 display(preprocessed_test_data.head(30))\n",
    "                preprocessed_test_data_noised = add_noise_by_trace(preprocessed_test_data, disturbed_trans_frac=uncert_level,\n",
    "                                         true_trans_prob=true_prob, expansion_min=expansion_min, expansion_max=expansion_max,\n",
    "                                         randomized_true_trans_prob = False, generate_probs_for_argmax = generate_probs_for_argmax,\n",
    "                                         gradual_noise=gradual_noise, alter_labels=alter_labels, swap_events=swap_events,\n",
    "                                         duplicate_acts=duplicate_acts, fraq=determ_noise_fraq)                  \n",
    "            else:\n",
    "                preprocessed_test_data_noised = add_noise(preprocessed_test_data, disturbed_trans_frac=uncert_level,\n",
    "                                         true_trans_prob=true_prob, expansion_min=expansion_min, expansion_max=expansion_max,\n",
    "                                         randomized_true_trans_prob = False, generate_probs_for_argmax = generate_probs_for_argmax)\n",
    "            \n",
    "            \n",
    "            return preprocessed_test_data_noised\n",
    "#            return preprocessed_test_data_noised\n",
    "\n",
    "#             print()\n",
    "#             print('dataset after noise')\n",
    "#             display(preprocessed_test_data_noised.head(30))\n",
    "            if all_probs_one is True:\n",
    "                preprocessed_test_data_noised = make_all_probs_one(preprocessed_test_data_noised)\n",
    "            \n",
    "            \n",
    "            if eval_lower_bound is True:\n",
    "                preprocessed_noised_determ = copy_dataframe(preprocessed_test_data_noised)\n",
    "                preprocessed_noised_determ = make_all_probs_one(preprocessed_noised_determ)\n",
    "                lower_bound_score = calc_conf_for_log(preprocessed_noised_determ, discovered_net, non_sync_move_penalty=non_sync_penalty, add_heuristic=add_heuristic)\n",
    "                lower_bound_scores.append(lower_bound_score)\n",
    "                \n",
    "\n",
    "#             non_sync_non_quiet_stochastic_alignment_length, sync_stochastic_alignment_length, \\\n",
    "#             non_sync_non_quiet_argmax_length, sync_argmax_alignment_length, stochastic_acc, \\\n",
    "#             argmax_acc, traces_length, stoc_conf_lst, fitness_lst = generate_statistics_for_dataset(preprocessed_test_data_noised, test_data, discovered_net, shortest_path_in_model, non_sync_penalty, add_heuristic)\n",
    "            stoc_conf_lst = generate_statistics_for_dataset(preprocessed_test_data_noised, test_data, discovered_net, shortest_path_in_model, non_sync_penalty, add_heuristic)\n",
    "\n",
    "#             mean_stochastic_avg = (np.array(stochastic_acc).dot(np.array(traces_length))) / sum(traces_length)\n",
    "#             mean_argmax_avg = (np.array(argmax_acc).dot(np.array(traces_length))) / sum(traces_length)\n",
    "\n",
    "#             non_sync_stochastic_avgs.append(mean(non_sync_non_quiet_stochastic_alignment_length))\n",
    "#             sync_stochastic_avgs.append(mean(sync_stochastic_alignment_length))\n",
    "#             non_sync_argmax_avgs.append(mean(non_sync_non_quiet_argmax_length))\n",
    "#             sync_argmax_avgs.append(mean(sync_argmax_alignment_length))\n",
    "#             stochastic_acc_avgs.append(mean_stochastic_avg)\n",
    "#             argmax_acc_avgs.append(mean_argmax_avg)\n",
    "            stochastic_conformance_avgs.append(mean(stoc_conf_lst))\n",
    "#             fitness_avgs.append(mean(fitness_lst))\n",
    "            \n",
    "            determ_preprocess_flag = 0\n",
    "            \n",
    "            print(f'true prob: {true_prob},  uncertainty_level: {uncert_level},  mean_stochastic_conformance: {mean(stoc_conf_lst)}')\n",
    "            print('--------------------------------------------------------------------------------------------------')\n",
    "            print()\n",
    "#     return non_sync_stochastic_avgs, sync_stochastic_avgs, non_sync_argmax_avgs, sync_argmax_avgs, \\\n",
    "#            stochastic_acc_avgs, argmax_acc_avgs, stochastic_conformance_avgs, fitness_avgs, lower_bound_scores \n",
    "    return stochastic_conformance_avgs, lower_bound_scores\n",
    "\n",
    "\n",
    "def generate_stats_dict_constant_stochastic_traces_frequency(history):\n",
    "    '''\n",
    "    Dictionary where keys are stochastic traces frequency and values are frequency\n",
    "    of alignments varied by the probability of the true transition\n",
    "    '''\n",
    "    \n",
    "    stats_dict = defaultdict(list)\n",
    "    stochastic_traces_frequency = 0\n",
    "    for i in range(21):\n",
    "        j=i\n",
    "        while j < 441:\n",
    "            stats_dict[stochastic_traces_frequency].append(history[j])\n",
    "            j += 21\n",
    "        \n",
    "        stochastic_traces_frequency += 0.05\n",
    "        stochastic_traces_frequency = round(stochastic_traces_frequency, 2)\n",
    "            \n",
    "    return stats_dict\n",
    "\n",
    "\n",
    "def filter_log(log, n_traces, max_len, random_selection=False, random_seed=42):\n",
    "    \n",
    "    cases_list = list(log['case:concept:name'].unique())\n",
    "    accepted_cases = []\n",
    "\n",
    "    for case in cases_list:\n",
    "        case_length = len(log[log['case:concept:name'] == case])\n",
    "\n",
    "        if case_length <= max_len:\n",
    "            accepted_cases.append(case) \n",
    "    \n",
    "    if random_selection:\n",
    "        random.seed(random_seed)\n",
    "        final_cases = random.sample(accepted_cases, n_traces)\n",
    "    \n",
    "    else:\n",
    "        final_cases = accepted_cases[:n_traces]\n",
    "        \n",
    "    filtered_df = log[log['case:concept:name'].isin(final_cases)]  \n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def prepare_df_cols_for_discovery(df):\n",
    "        df_copy = df.copy()\n",
    "        df_copy.loc[:, 'order'] = df_copy.groupby('case:concept:name').cumcount()\n",
    "        df_copy.loc[:, 'time:timestamp'] = pd.to_datetime(df_copy['order'])\n",
    "        \n",
    "        return df_copy\n",
    "\n",
    "\n",
    "def get_df_trace_lengths(df):\n",
    "    return df.groupby([\"case:concept:name\"])['concept:name'].count().reset_index(name='count')['count'].values\n",
    "\n",
    "\n",
    "def evaluate_conformance_cost(log, model, non_sync_penalty=1):\n",
    "    \n",
    "    cases_list = list(log['case:concept:name'].unique()) \n",
    "    conf_costs = []\n",
    "    for case in cases_list:\n",
    "        trace_case = log[log['case:concept:name'] == case]\n",
    "        trace_case = trace_case.drop('case:concept:name', axis=1)\n",
    "        trace_case = trace_case.reset_index(drop = True)\n",
    "#         display(trace_case)\n",
    "        trace_case_model = construct_trace_model(trace_case, non_sync_move_penalty=non_sync_penalty)\n",
    "        trace_alignment, trace_cost = model.conformance_checking(trace_case_model)\n",
    "#         print(f'trace cost: {trace_cost}')\n",
    "#         print(f'trace alignment: {trace_alignment}')\n",
    "        conf_costs.append(trace_cost)\n",
    "    \n",
    "    return conf_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-30T15:53:14.577775200Z",
     "start_time": "2024-08-30T15:53:14.564275600Z"
    }
   },
   "outputs": [],
   "source": [
    "def trace_self_loops_indices(trace_df):\n",
    "    trace_df_copy = copy.copy(trace_df)\n",
    "    trace_df_copy['concept:name_shifted'] = trace_df_copy['concept:name'].shift(1)\n",
    "    trace_df_copy['case:concept:name_shifted'] = trace_df_copy['case:concept:name'].shift(1)\n",
    "    trace_df_copy['is_duplicate'] = trace_df_copy.apply(check_duplicate_rows, axis=1)\n",
    "    return trace_df_copy['is_duplicate'].to_numpy().astype(np.bool)\n",
    "\n",
    "    \n",
    "def sample_n_traces(df, n_traces=10, random=True):\n",
    "    if random is False:\n",
    "        trace_cases = list(df['case:concept:name'].unique())[:n_traces]\n",
    "    else:\n",
    "        trace_cases = sample(list(df['case:concept:name'].unique()),n_traces)\n",
    "    return df[df['case:concept:name'].isin(trace_cases)]\n",
    "\n",
    "\n",
    "def remove_self_loops_in_trace(trace_df):\n",
    "    return trace_df[(trace_df['concept:name'] != trace_df.shift(1)['concept:name']) | (trace_df['case:concept:name'] != trace_df.shift(1)['case:concept:name'])]\n",
    "\n",
    "\n",
    "def remove_self_loops_in_dataset(log_df, return_self_loops_indices=False):\n",
    "    trace_cases = list(log_df['case:concept:name'].unique())\n",
    "    \n",
    "    new_traces_lst = []\n",
    "    self_loop_indices_lst = []\n",
    "    for trace_case in trace_cases:\n",
    "        trace = log_df[log_df['case:concept:name'] == trace_case]\n",
    "        curr_trace_self_loop_indices = trace_self_loops_indices(trace)\n",
    "        trace = remove_self_loops_in_trace(trace)\n",
    "        new_traces_lst.append(trace)\n",
    "        self_loop_indices_lst += list(curr_trace_self_loop_indices)\n",
    "    \n",
    "    new_log_df = pd.concat(new_traces_lst)\n",
    "    if return_self_loops_indices:\n",
    "        return new_log_df, np.array(self_loop_indices_lst)\n",
    "    return new_log_df\n",
    "\n",
    "\n",
    "def check_duplicate_rows(row):\n",
    "    if row['concept:name'] != row['concept:name_shifted'] or row['case:concept:name_shifted'] != row['case:concept:name_shifted']:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def argmax_sk_trace(trace_df):\n",
    "    trace_df['argmax_activity_label'] = trace_df.apply(get_max_prob_activity, axis=1)\n",
    "    return trace_df[['argmax_activity_label', 'case:concept:name']]\n",
    "\n",
    "\n",
    "def get_max_prob_activity(row):\n",
    "    max_idx = row['probs'].index(max(row['probs']))\n",
    "    max_prob_activity = row['concept:name'][max_idx]\n",
    "    return max_prob_activity\n",
    "\n",
    "def filter_softmax_matrice(sftm_mat, is_dup_bool_vec):\n",
    "    np_sftm_mat = sftm_mat.squeeze(0).cpu().numpy()\n",
    "    return np_sftm_mat[:,np.invert(is_dup_bool_vec)]\n",
    "\n",
    "\n",
    "def remove_loops_in_trace_and_matrice(trace_df, sftm_mat):\n",
    "    self_loops_indices = trace_self_loops_indices(trace_df)\n",
    "    no_loops_trace = remove_self_loops_in_trace(trace_df)\n",
    "    no_loops_sftm_mat = filter_softmax_matrice(sftm_mat, self_loops_indices)\n",
    "    return no_loops_trace, no_loops_sftm_mat\n",
    "\n",
    "\n",
    "def remove_loops_in_log_and_sftm_matrices_lst(log_df, sftm_mat_lst):\n",
    "    trace_cases = list(log_df['case:concept:name'].unique())\n",
    "    no_loops_trace_lst = []\n",
    "    no_loops_sftm_mat_lst = []\n",
    "    \n",
    "    for i, case in enumerate(trace_cases):\n",
    "        trace = log_df[log_df['case:concept:name'] == case]\n",
    "        loop_indices = trace_self_loops_indices(trace)\n",
    "        no_loops_trace = remove_self_loops_in_trace(trace)\n",
    "        no_loop_stmx_mat = filter_softmax_matrice(sftm_mat_lst[i], loop_indices)\n",
    "        no_loops_trace_lst.append(no_loops_trace)\n",
    "        no_loops_sftm_mat_lst.append(no_loop_stmx_mat)\n",
    "    \n",
    "    return pd.concat(no_loops_trace_lst), no_loops_sftm_mat_lst\n",
    "    \n",
    "\n",
    "def sfmx_mat_to_sk_trace(sftm_mat, case_num, round_precision=2):\n",
    "    if type(sftm_mat) is torch.Tensor:\n",
    "        sftm_mat = sftm_mat.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    activities_arr = np.arange(19)\n",
    "    df_prob_lst = []\n",
    "    df_activities_lst = []\n",
    "    di = activity_map_dict()\n",
    "    \n",
    "    for i in range(sftm_mat.shape[1]):\n",
    "        probs = np.round(sftm_mat[:,i], round_precision)\n",
    "        activities = list(activities_arr[np.nonzero(probs)])\n",
    "        activities = [di[str(act)] for act in activities]\n",
    "        df_prob_lst.append(list(probs[np.nonzero(probs)]))\n",
    "        df_activities_lst.append(activities)\n",
    "   \n",
    "    case_lst = [case_num] * sftm_mat.shape[1]\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "    {'concept:name': df_activities_lst,\n",
    "     'case:concept:name': case_lst,\n",
    "     'probs': df_prob_lst\n",
    "    })\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def argmax_sftmx_matrice(stmx_mat):\n",
    "    return np.argmax(stmx_mat, axis=0)  \n",
    "\n",
    "\n",
    "def train_test_log_split(log, n_traces, random_selection=False, random_seed=42):\n",
    "    \n",
    "    cases_list = list(log['case:concept:name'].unique())\n",
    "    assert len(cases_list) >= n_traces, \"Houston we've got a problem - more traces were demanded than there were available\"\n",
    "    \n",
    "    if random_selection:\n",
    "        random.seed(random_seed)\n",
    "        final_cases = random.sample(cases_list, n_traces)\n",
    "    \n",
    "    else:\n",
    "        final_cases = cases_list[:n_traces]\n",
    "        \n",
    "    train_df = log[log['case:concept:name'].isin(final_cases)]  \n",
    "    test_df = log[~log['case:concept:name'].isin(final_cases)]  \n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def select_stmx_mats_for_test(sfmx_mats, indices_lst):\n",
    "    if isinstance(indices_lst, pd.core.frame.DataFrame):\n",
    "        indices = sorted([int(num) for num in indices_lst['case:concept:name'].unique().tolist()])\n",
    "        \n",
    "    elif isinstance(indices_lst, pd.core.series.Series):\n",
    "        indices = sorted([int(num) for num in indices_lst.unique().tolist()])\n",
    "    \n",
    "    else:\n",
    "        indices = sorted([int(num) for num in indices_lst])\n",
    "    \n",
    "    return [sfmx_mats[i] for i in indices]\n",
    "\n",
    "\n",
    "def logarithmic(p):\n",
    "    return -np.log(p) / 2.4\n",
    "\n",
    "\n",
    "def shorten_df(df, stmx_lst, max_trace_len=4, remove_self_loops=True):\n",
    "    if remove_self_loops:\n",
    "        df, stmx = remove_loops_in_log_and_sftm_matrices_lst(df, stmx_lst)\n",
    "        \n",
    "    shortened_traces = []\n",
    "    cases = list(df['case:concept:name'].unique())\n",
    "    \n",
    "    for idx, trace_case in enumerate(cases):      \n",
    "        trace_df =  df[df['case:concept:name'] == trace_case] \n",
    "        trace_df = trace_df.head(max_trace_len)\n",
    "        shortened_traces.append(trace_df)\n",
    "        \n",
    "    stm_short = [mat[:,:max_trace_len] for mat in stmx]\n",
    "    return pd.concat(shortened_traces), stm_short\n",
    "\n",
    "\n",
    "def activity_map_dict():\n",
    "    di ={'0': 'cut_tomato',\n",
    "        '1': 'place_tomato_into_bowl',\n",
    "        '2': 'cut_cheese',\n",
    "        '3': 'place_cheese_into_bowl',\n",
    "        '4': 'cut_lettuce',\n",
    "        '5': 'place_lettuce_into_bowl',\n",
    "        '6': 'add_salt',\n",
    "        '7': 'add_vinegar',\n",
    "        '8': 'add_oil',\n",
    "        '9': 'add_pepper',\n",
    "        '10': 'mix_dressing',\n",
    "        '11': 'peel_cucumber',\n",
    "        '12': 'cut_cucumber',\n",
    "        '13': 'place_cucumber_into_bowl',\n",
    "        '14': 'add_dressing',\n",
    "        '15': 'mix_ingredients',\n",
    "        '16': 'serve_salad_onto_plate',\n",
    "        '17': 'action_start',\n",
    "        '18': 'action_end'}\n",
    "    \n",
    "    return di\n",
    "\n",
    "\n",
    "def compare_stochastic_vs_argmax_no_loops(df, softmax_lst, net=None, init_marking=None, final_marking=None, n_train_traces=10, cost_function=None, round_precision=2, random_trace_selection=True, random_seed=42, non_sync_penalty=1, remove_self_loops=True, max_trace_len=None):\n",
    "    \n",
    "    if cost_function is None:\n",
    "        cost_function=logarithmic\n",
    "    \n",
    "    if remove_self_loops and max_trace_len is None:\n",
    "        df_no_loops, stmx_lst_no_loops = remove_loops_in_log_and_sftm_matrices_lst(df, softmax_lst)\n",
    "    \n",
    "    if max_trace_len is not None:\n",
    "        df_no_loops, stmx_lst_no_loops = shorten_df(df, softmax_lst, max_trace_len=max_trace_len)\n",
    "        \n",
    "    df_train, df_test = train_test_log_split(df_no_loops,n_traces=n_train_traces,random_selection=random_trace_selection,random_seed=random_seed)\n",
    "    stmx_matrices_test = select_stmx_mats_for_test(stmx_lst_no_loops, df_test)\n",
    "    \n",
    "    if net is None:\n",
    "        net, init_marking, final_marking = pm4py.discover_petri_net_inductive(df_train)\n",
    "#     model = from_discovered_model_to_PetriNet(net, non_sync_move_penalty=non_sync_penalty, cost_function=cost_function)\n",
    "#     gviz = pn_visualizer.apply(net, init_marking, final_marking)\n",
    "#     pn_visualizer.view(gviz)\n",
    "\n",
    "    model = from_discovered_model_to_PetriNet(net, non_sync_move_penalty=non_sync_penalty)\n",
    "    test_traces_cases = list(df_test['case:concept:name'].unique())\n",
    "    stochastic_acc_lst = []\n",
    "    argmax_acc_lst = []\n",
    "    for idx, trace_case in enumerate(test_traces_cases):\n",
    "    \n",
    "        true_trace_df =  df_test[df_test['case:concept:name'] == trace_case]\n",
    "        stochastic_trace_df = sfmx_mat_to_sk_trace(stmx_matrices_test[idx], trace_case, round_precision=round_precision)\n",
    "#         display(stochastic_trace_df)  \n",
    "        stochastic_acc, argmax_acc = compare_argmax_and_stochastic_alignments(stochastic_trace_df, true_trace_df, model, non_sync_penalty=non_sync_penalty)\n",
    "        stochastic_acc_lst.append(stochastic_acc)\n",
    "        argmax_acc_lst.append(argmax_acc)\n",
    "\n",
    "    return stochastic_acc_lst, argmax_acc_lst\n",
    "\n",
    "\n",
    "def compare_stochastic_vs_argmax_random_indices(df, softmax_lst, cost_function=None, n_train_traces=10, n_indices=100, round_precision=2, random_trace_selection=True, random_seed=42, non_sync_penalty=1):\n",
    "   \n",
    "    if cost_function is None:\n",
    "        cost_function=logarithmic\n",
    "\n",
    "    df_random_indices, stmx_lst_random_indices = select_random_indices_in_log_and_sftm_matrices_lst(df, softmax_lst, n_indices) \n",
    "    df_train, df_test = train_test_log_split(df_random_indices, n_traces=n_train_traces, random_selection=random_trace_selection, random_seed=random_seed)\n",
    "    stmx_matrices_test = select_stmx_mats_for_test(stmx_lst_random_indices, df_test)\n",
    "    \n",
    "    df_train = prepare_df_cols_for_discovery(df_train)\n",
    "    net, init_marking, final_marking = pm4py.discover_petri_net_inductive(df_train)\n",
    "#     model = from_discovered_model_to_PetriNet(net, non_sync_move_penalty=non_sync_penalty, cost_function=cost_function)\n",
    "    model = from_discovered_model_to_PetriNet(net, non_sync_move_penalty=non_sync_penalty)\n",
    "\n",
    "    test_traces_cases = list(df_test['case:concept:name'].unique())\n",
    "    stochastic_acc_lst = []\n",
    "    argmax_acc_lst = []\n",
    "    for idx, trace_case in enumerate(test_traces_cases):\n",
    "        true_trace_df =  df_test[df_test['case:concept:name'] == trace_case].reset_index(drop=True)\n",
    "        stochastic_trace_df = sfmx_mat_to_sk_trace(stmx_matrices_test[idx], trace_case, round_precision=round_precision)\n",
    "#         display(true_trace_df)\n",
    "#         display(stochastic_trace_df)  \n",
    "        stochastic_acc, argmax_acc = compare_argmax_and_stochastic_alignments(stochastic_trace_df, true_trace_df, model, non_sync_penalty=non_sync_penalty)\n",
    "        stochastic_acc_lst.append(stochastic_acc)\n",
    "        argmax_acc_lst.append(argmax_acc)\n",
    "      \n",
    "    return stochastic_acc_lst, argmax_acc_lst\n",
    "        \n",
    "        \n",
    "def select_random_indices_in_log_and_sftm_matrices_lst(log_df, sftm_mat_lst, n_indices = 100):\n",
    "    trace_cases = list(log_df['case:concept:name'].unique())\n",
    "    filtered_trace_lst = []\n",
    "    filtered_sftm_mat_lst = []\n",
    "    \n",
    "    for i, case in enumerate(trace_cases):\n",
    "        trace = log_df[log_df['case:concept:name'] == case]\n",
    "        np_sftm_mat = sftm_mat_lst[i].squeeze(0).cpu().numpy()\n",
    "        if n_indices is None:\n",
    "            n_indices = np_sftm_mat.shape[1]\n",
    "        selected_indices = sample(list(range(np_sftm_mat.shape[1])), n_indices)\n",
    "        selected_indices_bool = np.zeros(np_sftm_mat.shape[1], dtype=bool)\n",
    "        np.add.at(selected_indices_bool, selected_indices, 1) \n",
    "        sftm_filtered = np_sftm_mat[:,selected_indices_bool]\n",
    "        trace_df_filtered = trace[selected_indices_bool]\n",
    "        filtered_trace_lst.append(trace_df_filtered)\n",
    "        filtered_sftm_mat_lst.append(sftm_filtered)\n",
    "    return pd.concat(filtered_trace_lst), filtered_sftm_mat_lst\n",
    "\n",
    "\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else: return super().find_class(module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-30T15:53:14.608774200Z",
     "start_time": "2024-08-30T15:53:14.572274500Z"
    }
   },
   "outputs": [],
   "source": [
    "# PATH = 'C:\\\\Users\\\\User\\\\Jupyter Projects\\\\Research_2\\\\Datasets\\\\Video'\n",
    "# os.chdir(PATH)\n",
    "\n",
    "\n",
    "with open('data/pickles/salads_softmax_lst.pickle', 'rb') as handle:\n",
    "    softmax_lst = CPU_Unpickler(handle).load()\n",
    "    \n",
    "    \n",
    "with open('data/pickles/salads_target_lst.pickle', 'rb') as handle:\n",
    "    target_lst = CPU_Unpickler(handle).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-30T15:53:14.742774400Z",
     "start_time": "2024-08-30T15:53:14.609274400Z"
    }
   },
   "outputs": [],
   "source": [
    "concant_tensor_lst = []\n",
    "concat_idx_lst = []\n",
    "\n",
    "for i, tensor in enumerate(target_lst):\n",
    "    tensor_lst = tensor.tolist()\n",
    "    tensor_lst = [str(elem) for elem in tensor_lst]\n",
    "    idx_lst = [str(i)] * len(tensor_lst)\n",
    "    concant_tensor_lst += tensor_lst\n",
    "    concat_idx_lst += idx_lst\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame(\n",
    "    {'concept:name': concant_tensor_lst,\n",
    "     'case:concept:name': concat_idx_lst\n",
    "    })\n",
    "\n",
    "\n",
    "di = activity_map_dict()\n",
    "df[\"concept:name\"] = df[\"concept:name\"].replace(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-30T15:53:15.260276200Z",
     "start_time": "2024-08-30T15:53:14.748277900Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_node_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 9\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# This function compares the accuracy between SKTR and the heuristic 'Argmax' which selects the activity with the highest probability in each timestamp\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# The arguments for the function are as follows: df is a pandas DataFrame with the ground truth labels with two columns: 'case:concept:name' which represents the case of the trace\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# and 'concept:name' which represents the activity label\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Results may be suboptimal for sampling only part of the indices since the discovered process model may not contain all the activities\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# the function can receive other parameters - look in the function definition\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m stochastic_acc, argmax_acc \u001B[38;5;241m=\u001B[39m compare_stochastic_vs_argmax_random_indices(df, softmax_lst, n_indices\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)\n",
      "Cell \u001B[1;32mIn[12], line 248\u001B[0m, in \u001B[0;36mcompare_stochastic_vs_argmax_random_indices\u001B[1;34m(df, softmax_lst, cost_function, n_train_traces, n_indices, round_precision, random_trace_selection, random_seed, non_sync_penalty)\u001B[0m\n\u001B[0;32m    245\u001B[0m         stochastic_trace_df \u001B[38;5;241m=\u001B[39m sfmx_mat_to_sk_trace(stmx_matrices_test[idx], trace_case, round_precision\u001B[38;5;241m=\u001B[39mround_precision)\n\u001B[0;32m    246\u001B[0m \u001B[38;5;66;03m#         display(true_trace_df)\u001B[39;00m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;66;03m#         display(stochastic_trace_df)  \u001B[39;00m\n\u001B[1;32m--> 248\u001B[0m         stochastic_acc, argmax_acc \u001B[38;5;241m=\u001B[39m compare_argmax_and_stochastic_alignments(stochastic_trace_df, true_trace_df, model, non_sync_penalty\u001B[38;5;241m=\u001B[39mnon_sync_penalty)\n\u001B[0;32m    249\u001B[0m         stochastic_acc_lst\u001B[38;5;241m.\u001B[39mappend(stochastic_acc)\n\u001B[0;32m    250\u001B[0m         argmax_acc_lst\u001B[38;5;241m.\u001B[39mappend(argmax_acc)\n",
      "Cell \u001B[1;32mIn[11], line 588\u001B[0m, in \u001B[0;36mcompare_argmax_and_stochastic_alignments\u001B[1;34m(stochastic_trace_df, true_trace_df, model, non_sync_penalty, add_heuristic)\u001B[0m\n\u001B[0;32m    585\u001B[0m     case_trace_stochastic_model \u001B[38;5;241m=\u001B[39m construct_trace_model(df_stochastic, non_sync_penalty)\n\u001B[0;32m    587\u001B[0m \u001B[38;5;66;03m#     case_trace_stochastic_model = construct_trace_model(df_stochastic, non_sync_penalty, add_heuristic=add_heuristic)\u001B[39;00m\n\u001B[1;32m--> 588\u001B[0m     stochastic_alignment \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mconformance_checking(case_trace_stochastic_model)\n\u001B[0;32m    589\u001B[0m \u001B[38;5;66;03m#     print('True trace is:')\u001B[39;00m\n\u001B[0;32m    590\u001B[0m \u001B[38;5;66;03m#     display(true_trace_df)\u001B[39;00m\n\u001B[0;32m    591\u001B[0m \u001B[38;5;66;03m#     print()\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    596\u001B[0m \u001B[38;5;66;03m#     print(f'The cost of the alignment is: {stochastic_alignment[1]}')\u001B[39;00m\n\u001B[0;32m    597\u001B[0m \u001B[38;5;66;03m#     print()\u001B[39;00m\n\u001B[0;32m    598\u001B[0m     argmax_trace \u001B[38;5;241m=\u001B[39m argmax_stochastic_trace(stochastic_trace_df)\n",
      "Cell \u001B[1;32mIn[10], line 1019\u001B[0m, in \u001B[0;36mPetriNet.conformance_checking\u001B[1;34m(self, trace_model, hist_prob_dict, lamda)\u001B[0m\n\u001B[0;32m   1017\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconformance_checking\u001B[39m(\u001B[38;5;28mself\u001B[39m, trace_model, hist_prob_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, lamda\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m):\n\u001B[0;32m   1018\u001B[0m     sync_prod \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconstruct_synchronous_product(trace_model)      \n\u001B[1;32m-> 1019\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sync_prod\u001B[38;5;241m.\u001B[39m_dijkstra_no_rg_construct(hist_prob_dict, lamda\u001B[38;5;241m=\u001B[39mlamda)\n",
      "Cell \u001B[1;32mIn[10], line 1072\u001B[0m, in \u001B[0;36mPetriNet._dijkstra_no_rg_construct\u001B[1;34m(self, prob_dict, lamda, return_final_marking)\u001B[0m\n\u001B[0;32m   1070\u001B[0m distance_min_heap \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m   1071\u001B[0m heapify(distance_min_heap) \n\u001B[1;32m-> 1072\u001B[0m curr_node \u001B[38;5;241m=\u001B[39m search_node_new(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_mark, dist\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m   1073\u001B[0m heappush(distance_min_heap, curr_node)        \n\u001B[0;32m   1074\u001B[0m marking_distance_dict \u001B[38;5;241m=\u001B[39m {}\n",
      "\u001B[1;31mNameError\u001B[0m: name 'search_node_new' is not defined"
     ]
    }
   ],
   "source": [
    "# This function compares the accuracy between SKTR and the heuristic 'Argmax' which selects the activity with the highest probability in each timestamp\n",
    "# The arguments for the function are as follows: df is a pandas DataFrame with the ground truth labels with two columns: 'case:concept:name' which represents the case of the trace\n",
    "# and 'concept:name' which represents the activity label\n",
    "# the softmax_lst is the list that holds the sotmax matrices i.e., the probabilities for each trace. Rows are the different possible activities and columns are the sequential timestamps of the trace (possibly each matrice here is a pytorch tensor but should also work for numpy matrices, maybe small adjustment would need to be made)\n",
    "# n_indices are the number of timestamp you want to sample from the matrices. None = sample all the matrice i.e., restore the whole trace\n",
    "# Results may be suboptimal for sampling only part of the indices since the discovered process model may not contain all the activities\n",
    "# the function can receive other parameters - look in the function definition\n",
    "\n",
    "stochastic_acc, argmax_acc = compare_stochastic_vs_argmax_random_indices(df, softmax_lst, n_indices=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-08-30T15:53:15.255774600Z"
    }
   },
   "outputs": [],
   "source": [
    "mean(stochastic_acc), mean(argmax_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-30T15:53:15.257275800Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
