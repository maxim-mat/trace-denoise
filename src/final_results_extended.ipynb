{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-03T20:50:44.357082Z",
     "start_time": "2026-01-03T20:50:36.204934Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import plotly.graph_objects as go\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from scipy.stats import entropy\n",
    "import sklearn.metrics as metrics\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from denoisers.ConditionalUnetDenoiser import ConditionalUnetDenoiser\n",
    "from denoisers.ConditionalUnetMatrixDenoiser import ConditionalUnetMatrixDenoiser\n",
    "from utils.graph_utils import get_process_model_reachability_graph_transition_matrix, \\\n",
    "    get_process_model_petri_net_transition_matrix, get_process_model_reachability_graph_transition_multimatrix\n",
    "from utils.pm_utils import discover_dk_process, remove_duplicates_dataset, pad_to_multiple_of_n\n",
    "from utils.Config import Config\n",
    "from dataset.dataset import SaladsDataset\n",
    "from ddpm.ddpm_multinomial import Diffusion\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.pm_utils import conformance_measure\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import sklearn.metrics as metrics\n",
    "from src.denoisers.ConditionalUnetGraphDenoiser import ConditionalUnetGraphDenoiser\n",
    "from src.utils.graph_utils import prepare_process_model_for_gnn, \\\n",
    "    get_process_model_reachability_graph_transition_multimatrix"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T21:09:45.830931Z",
     "start_time": "2026-01-03T21:09:45.811498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_beta_params(low, high, middle):\n",
    "    \"\"\"\n",
    "    Determines Beta parameters such that when sampling X ~ Beta(a, b)\n",
    "    and scaling alpha = low + (high - low) * X, we have P(alpha < 0.5) ≈ middle.\n",
    "    Here we fix a = 1 and solve for b.\n",
    "\n",
    "    Parameters:\n",
    "      low    : lower bound of the target range.\n",
    "      high   : upper bound of the target range.\n",
    "      middle : desired probability that alpha is less than 0.5.\n",
    "\n",
    "    Returns:\n",
    "      (a, b) : tuple of Beta parameters.\n",
    "    \"\"\"\n",
    "    a = 1.0\n",
    "    q = (0.5 - low) / (high - low)\n",
    "    b = np.log(1 - middle) / np.log(1 - q)\n",
    "    return a, b\n",
    "\n",
    "\n",
    "def sample_noise_mask(T,\n",
    "                      p_clean_to_noisy=0.01,\n",
    "                      p_noisy_to_clean=0.1):\n",
    "    \"\"\"\n",
    "    Returns a boolean mask of length T.  True = noisy state, False = clean.\n",
    "    The expected run‐length in noisy state is ~1/p_noisy_to_clean,\n",
    "    and the long‐run fraction of time spent in noisy state is\n",
    "        π_noisy = p_clean_to_noisy / (p_clean_to_noisy + p_noisy_to_clean)\n",
    "    \"\"\"\n",
    "    state = False  # start clean\n",
    "    mask = np.zeros(T, dtype=bool)\n",
    "    for t in range(T):\n",
    "        if not state:\n",
    "            # clean → noisy?\n",
    "            if np.random.rand() < p_clean_to_noisy:\n",
    "                state = True\n",
    "        else:\n",
    "            # noisy → clean?\n",
    "            if np.random.rand() < p_noisy_to_clean:\n",
    "                state = False\n",
    "        mask[t] = state\n",
    "    return mask\n",
    "\n",
    "\n",
    "def generate_locked_bursty_trace(target,\n",
    "                                 low=0.05, high=1.0, middle=0.8,\n",
    "                                 noise_level_clean=0.0,\n",
    "                                 noise_level_noisy=0.6,\n",
    "                                 p_c2n=0.005, p_n2c=0.02):\n",
    "    \"\"\"\n",
    "    Same as before, but in each noisy segment we pick a single wrong class j\n",
    "    and blend (1-noise_level)*one_hot(j) + noise_level*Dirichlet_noise,\n",
    "    so argmax stays j throughout the burst.\n",
    "    \"\"\"\n",
    "    T, K = target.shape\n",
    "    # 1) sample the clean/noisy mask\n",
    "    noisy_mask = sample_noise_mask(T, p_c2n, p_n2c)\n",
    "\n",
    "    # 2) precompute your Beta->Dirichlet concentration if you like\n",
    "    a, b = get_beta_params(low, high, middle)\n",
    "\n",
    "    noisy_trace = []\n",
    "    t = 0\n",
    "    while t < T:\n",
    "        if not noisy_mask[t]:\n",
    "            # clean: just keep original p exactly\n",
    "            p = target[t]\n",
    "            lvl = noise_level_clean\n",
    "            # fresh Dirichlet noise around uniform\n",
    "            alpha_vec = np.ones(K) * np.random.beta(a, b)\n",
    "            noise = np.random.dirichlet(alpha_vec)\n",
    "            perturbed = (1 - lvl) * p + lvl * noise\n",
    "            if np.any(np.isnan(perturbed)):\n",
    "                    perturbed = p\n",
    "            noisy_trace.append(perturbed / perturbed.sum())\n",
    "            t += 1\n",
    "        else:\n",
    "            # find the length of this noisy run\n",
    "            start = t\n",
    "            while t < T and noisy_mask[t]:\n",
    "                t += 1\n",
    "            end = t\n",
    "\n",
    "            # pick one wrong class j for the _whole_ run\n",
    "            true_label = target[start].argmax()\n",
    "            # choose j != true_label\n",
    "            candidates = list(range(K))\n",
    "            candidates.remove(true_label)\n",
    "            j = np.random.choice(candidates)\n",
    "\n",
    "            # now fill in each time‐step in [start, end)\n",
    "            for _ in range(start, end):\n",
    "                # draw fresh Dirichlet around uniform base (or you can weight it)\n",
    "                alpha_vec = np.ones(K) * np.random.beta(a, b)\n",
    "                noise = np.random.dirichlet(alpha_vec)\n",
    "\n",
    "                # mix with constant one_hot(j)\n",
    "                lvl = noise_level_noisy\n",
    "                one_hot_j = np.eye(K)[j]\n",
    "                perturbed = (1 - lvl) * one_hot_j + lvl * noise\n",
    "                # it already sums to 1, so no need to renormalize\n",
    "                if np.any(np.isnan(perturbed)):\n",
    "                    perturbed = one_hot_j\n",
    "                noisy_trace.append(perturbed)\n",
    "\n",
    "    return np.stack(noisy_trace)\n",
    "\n",
    "\n",
    "def visualize_traces(x, y, z, names=None):\n",
    "    if names is None:\n",
    "        names = ['Original', 'Argmax', 'Reconstructed']\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=list(range(len(x))), y=x, mode='lines', name=names[0],\n",
    "                             line=dict(color='blue', dash='solid')))\n",
    "    fig.add_trace(go.Scatter(x=list(range(len(y))), y=y, mode='lines', name=names[1],\n",
    "                             line=dict(color='red', dash='dash')))\n",
    "    fig.add_trace(go.Scatter(x=list(range(len(z))), y=z, mode='lines', name=names[2],\n",
    "                             line=dict(color='green', dash='dot')))\n",
    "    return fig\n",
    "\n",
    "\n",
    "def average_entropy_data(data):\n",
    "    return np.mean([np.mean(entropy(x, axis=1)) for x in data])\n",
    "\n",
    "\n",
    "def load_experiment_config(target_dir):\n",
    "    config_path = os.path.join(target_dir, \"cfg.json\")\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, \"r\") as f:\n",
    "            return Config(**json.load(f))\n",
    "    else:\n",
    "        st.warning(\"Configuration file not found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_experiment_data_and_model(target_dir, cfg):\n",
    "    with open(cfg.data_path, \"rb\") as f:\n",
    "        base_dataset = pkl.load(f)\n",
    "    dataset = SaladsDataset(base_dataset['target'], base_dataset['stochastic'])\n",
    "    train_dataset, test_dataset = train_test_split(dataset, train_size=cfg.train_percent, shuffle=True,\n",
    "                                                   random_state=cfg.seed)\n",
    "    rg_transition_matrix = torch.randn((cfg.num_classes, 2, 2)).to(cfg.device)\n",
    "    dk_process_model, dk_init_marking, dk_final_marking = discover_dk_process(train_dataset, cfg,\n",
    "                                                                              preprocess=remove_duplicates_dataset)\n",
    "    if cfg.enable_matrix:\n",
    "        if cfg.matrix_type == \"pm\":\n",
    "            rg_nx, rg_transition_matrix = get_process_model_petri_net_transition_matrix(dk_process_model,\n",
    "                                                                                        dk_init_marking,\n",
    "                                                                                        dk_final_marking)\n",
    "            rg_transition_matrix = torch.tensor(rg_transition_matrix, device=cfg.device).unsqueeze(0).float()\n",
    "        elif cfg.matrix_type == \"rg\":\n",
    "            rg_nx, rg_transition_matrix = get_process_model_reachability_graph_transition_multimatrix(dk_process_model,\n",
    "                                                                                                      dk_init_marking)\n",
    "            rg_transition_matrix = torch.tensor(rg_transition_matrix, device=cfg.device).float()\n",
    "        rg_transition_matrix = pad_to_multiple_of_n(rg_transition_matrix)\n",
    "    if cfg.enable_gnn:\n",
    "        pm_nx_data = prepare_process_model_for_gnn(dk_process_model, dk_init_marking, dk_final_marking,\n",
    "                                                   cfg).to(cfg.device)\n",
    "\n",
    "    diffuser = Diffusion(noise_steps=cfg.num_timesteps, device=cfg.device)\n",
    "    if cfg.enable_matrix:\n",
    "        denoiser = ConditionalUnetMatrixDenoiser(in_ch=cfg.num_classes, out_ch=cfg.num_classes,\n",
    "                                                 max_input_dim=dataset.sequence_length,\n",
    "                                                 transition_dim=rg_transition_matrix.shape[-1],\n",
    "                                                 gamma=cfg.gamma,\n",
    "                                                 matrix_out_channels=rg_transition_matrix.shape[0],\n",
    "                                                 device=cfg.device).to(cfg.device).float()\n",
    "    elif cfg.enable_gnn:\n",
    "        denoiser = ConditionalUnetGraphDenoiser(in_ch=cfg.num_classes, out_ch=cfg.num_classes,\n",
    "                                                max_input_dim=dataset.sequence_length,\n",
    "                                                num_nodes=pm_nx_data.num_nodes,\n",
    "                                                graph_data=pm_nx_data,\n",
    "                                                embedding_dim=128, hidden_dim=128, pooling=cfg.gnn_pooling).to(cfg.device).float()\n",
    "    else:\n",
    "        denoiser = ConditionalUnetDenoiser(in_ch=cfg.num_classes, out_ch=cfg.num_classes,\n",
    "                                           max_input_dim=dataset.sequence_length,\n",
    "                                           device=cfg.device).to(cfg.device).float()\n",
    "    ckpt_path = os.path.join(target_dir, \"best.ckpt\")\n",
    "    denoiser.load_state_dict(torch.load(ckpt_path, map_location=cfg.device)['model_state'])\n",
    "    final_res_path = os.path.join(target_dir, \"final_results.json\")\n",
    "    if os.path.exists(final_res_path):\n",
    "        with open(final_res_path, \"r\") as f:\n",
    "            final_res = json.load(f)\n",
    "\n",
    "    return (train_dataset, test_dataset, dk_process_model, dk_init_marking, dk_final_marking, rg_transition_matrix,\n",
    "            diffuser, denoiser, final_res)\n",
    "\n",
    "\n",
    "def evaluate_dataset_new(denoiser, diffuser, cfg, loader):\n",
    "    pad_token = cfg.num_classes - 1\n",
    "    denoised = []\n",
    "    gt = []\n",
    "    for x, y in loader:\n",
    "        x = x.permute(0, 2, 1).to(cfg.device).float()\n",
    "        y = y.permute(0, 2, 1).to(cfg.device).float()\n",
    "        x_hat, matrix_hat, loss, seq_loss, mat_loss = \\\n",
    "            diffuser.sample_with_matrix(denoiser, y.shape[0], cfg.num_classes, denoiser.max_input_dim,\n",
    "                                        rg_transition_matrix.shape[-1], rg_transition_matrix, x, y,\n",
    "                                        cfg.predict_on)\n",
    "        denoised.append(x_hat)\n",
    "        gt.append(x)\n",
    "    denoised = torch.cat(denoised, dim=0)\n",
    "    gt = torch.cat(gt, dim=0)\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for x_0, y in zip(gt, denoised):\n",
    "        x_tokens = torch.argmax(x_0, dim=0)\n",
    "        y_tokens = torch.argmax(torch.softmax(y, dim=0).transpose(0, 1), dim=1)\n",
    "        x_list.append(np.array(x_tokens[x_tokens != pad_token].cpu()))\n",
    "        y_list.append(np.array(y_tokens[x_tokens != pad_token].cpu()))\n",
    "\n",
    "    acc = np.mean([accuracy_score(x, y) for x, y in zip(x_list, y_list)])\n",
    "    rec = np.mean([recall_score(x, y, average='macro', zero_division=0) for x, y in zip(x_list, y_list)])\n",
    "    pre = np.mean([precision_score(x, y, average='macro', zero_division=0) for x, y in zip(x_list, y_list)])\n",
    "\n",
    "    return gt, denoised, acc, pre, rec"
   ],
   "id": "7c0c1dce2b5e3f1a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T17:17:23.039406Z",
     "start_time": "2026-01-02T17:17:23.012991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path = r\"../data/pickles/50_salads_unified.pkl\"\n",
    "with open(data_path, \"rb\") as f:\n",
    "    data = pkl.load(f)\n",
    "\n",
    "target, source = data['target'], data['stochastic']"
   ],
   "id": "3df67877324ca1ae",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T17:17:23.094404Z",
     "start_time": "2026-01-02T17:17:23.048205Z"
    }
   },
   "cell_type": "code",
   "source": "np.mean([metrics.accuracy_score(np.argmax(t, axis=1), np.argmax(s, axis=1)) for t, s in zip(target, source)])",
   "id": "ffa9fbd24155be97",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8259146587893127"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T17:17:23.173871Z",
     "start_time": "2026-01-02T17:17:23.106662Z"
    }
   },
   "cell_type": "code",
   "source": "average_entropy_data(source)",
   "id": "699a8b8441e3543c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24932393"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T17:18:59.024058Z",
     "start_time": "2026-01-02T17:18:58.685091Z"
    }
   },
   "cell_type": "code",
   "source": "noise_argmax_accuracies = [np.mean([metrics.accuracy_score(np.argmax(t, axis=1), np.argmax(s, axis=1)) for t, s in zip(target, stochastic_ds)]) for stochastic_ds in noisy_datasets]",
   "id": "f62d6e79d631cf46",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T17:18:59.809776Z",
     "start_time": "2026-01-02T17:18:59.028339Z"
    }
   },
   "cell_type": "code",
   "source": "noise_entropies = [average_entropy_data(stochastic_ds) for stochastic_ds in noisy_datasets]",
   "id": "c7a2b40b58494380",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T17:19:01.423305Z",
     "start_time": "2026-01-02T17:18:59.814447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_dir = r\"D:\\Projects\\trace-denoise\\final_runs_extended\\50_salads_free\"\n",
    "cfg = load_experiment_config(target_dir)\n",
    "cfg.device = \"cuda:0\"\n",
    "train_dataset, test_dataset, dk_process_model, dk_init_marking, dk_final_marking, rg_transition_matrix, diffuser, denoiser, final_res = load_experiment_data_and_model(target_dir, cfg)"
   ],
   "id": "dc720ed651c74d25",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T17:35:18.566184Z",
     "start_time": "2026-01-02T17:19:01.558248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accumulators = []\n",
    "results_list = []\n",
    "for stochastic_ds in tqdm(noisy_datasets):\n",
    "    noisy_dataset = SaladsDataset(target, stochastic_ds)\n",
    "    _, noisy_test_dataset = train_test_split(noisy_dataset, train_size=cfg.train_percent, random_state=cfg.seed)\n",
    "    noisy_test_loader = DataLoader(noisy_test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "    accumulator, results = evaluate_dataset(denoiser, diffuser, cfg, noisy_test_loader)\n",
    "    accumulators.append(accumulator)\n",
    "    results_list.append(results)"
   ],
   "id": "e65328677b717d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "541e7b99e3bb40708f8423d7f689b886"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a67b36f0de6423383fa6e1889931934"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "049fa6267d704990aae7cf396afc6fa9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c698eb2671a742dc9bcf39b7b2ca869e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "928bdb31032c421fbba019266b7169ec"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d7f4f93ee0c47e2a22aa1e3ce03e58f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d748dfe12a74477f8d5971f1fda4ff46"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea6cca4647ac482f94b428a58f0b0ac3"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3fd4cf7e1334e6cafe65853bb9dbd95"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b35c408c336a4973bca77485576413c2"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "705e71f33ab847d5b23260fcaccb12a0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bddeff917264275895470631bb51096"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d26c64778cc4eedaf835877c23c5925"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49b5886dbf424bd79a9b7351ef8d39a7"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T17:35:19.350608Z",
     "start_time": "2026-01-02T17:35:18.791847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accs, pre, rec = [], [], []\n",
    "pad_token = 19\n",
    "for accumulator in accumulators:\n",
    "    i_acc, i_pre, i_rec = [], [], []\n",
    "    for x, y, x_hat in zip(accumulator['x'], accumulator['y'], accumulator['x_hat']):\n",
    "        for tdk, tsk, that in zip(x, y, x_hat):\n",
    "            dk = torch.argmax(tdk, dim=0).cpu().numpy()\n",
    "            hat = torch.argmax(torch.softmax(that, dim=1), dim=1).cpu().numpy()\n",
    "            i_acc.append(metrics.accuracy_score(dk[dk != pad_token], hat[dk != pad_token]))\n",
    "            i_pre.append(metrics.precision_score(dk[dk != pad_token], hat[dk != pad_token], average='macro', zero_division=0))\n",
    "            i_rec.append(metrics.recall_score(dk[dk != pad_token], hat[dk != pad_token], average='macro', zero_division=0))\n",
    "    accs.append(np.mean(i_acc))\n",
    "    pre.append(np.mean(i_pre))\n",
    "    rec.append(np.mean(i_rec))"
   ],
   "id": "e4993e2661f4c7a7",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T18:15:10.726064Z",
     "start_time": "2026-01-02T18:00:12.093656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evals_new = []\n",
    "accs_new = []\n",
    "pre_new = []\n",
    "for stochastic_ds in tqdm(noisy_datasets):\n",
    "    noisy_dataset = SaladsDataset(target, stochastic_ds)\n",
    "    _, noisy_test_dataset = train_test_split(noisy_dataset, train_size=cfg.train_percent, random_state=cfg.seed)\n",
    "    noisy_test_loader = DataLoader(noisy_test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "    gt, denoised, acc, pre, rec = evaluate_dataset_new(denoiser, diffuser, cfg, noisy_test_loader)\n",
    "    evals_new.append((gt, denoised, acc, pre, rec))\n",
    "    accs_new.append(acc)\n",
    "    pre_new.append(pre)"
   ],
   "id": "29482dc7e9edf71e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ede141599a2c42d588ca457a139e2ddc"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T20:54:35.509594Z",
     "start_time": "2026-01-03T20:54:35.505907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clean_levels = np.arange(0.9, 0.6, -0.025)\n",
    "noise_levels = np.arange(0.2, 0.525, 0.025)\n",
    "global_levels = np.arange(0.2, 0.33, 0.01)"
   ],
   "id": "d37617150eb40401",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T21:25:41.697903Z",
     "start_time": "2026-01-03T21:09:58.148248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "noisy_datasets_set = set()\n",
    "\n",
    "for exp in tqdm(os.listdir(r\"D:\\Projects\\trace-denoise\\final_runs_extended\")):\n",
    "    target_dir = os.path.join(r\"D:\\Projects\\trace-denoise\\final_runs_extended\", exp)\n",
    "    cfg = load_experiment_config(target_dir)\n",
    "    cfg.device = \"cuda:0\"\n",
    "    if cfg.data_path not in noisy_datasets_set:\n",
    "        with open(cfg.data_path, \"rb\") as f:\n",
    "            data = pkl.load(f)\n",
    "        target, source = data['target'], data['stochastic']\n",
    "        noisy_datasets = [[np.array(generate_locked_bursty_trace(t, low=0.05, high=3, middle=0.5,\n",
    "                                                         noise_level_clean=lv_global,\n",
    "                                                         noise_level_noisy=lv_global,\n",
    "                                                         p_c2n=lv_noise,\n",
    "                                                         p_n2c=lv_clean)) for t in target] for lv_noise, lv_clean, lv_global in zip(noise_levels, clean_levels, global_levels)]\n",
    "        noisy_datasets_set.add(cfg.data_path)\n",
    "        with open(f\"{Path(cfg.data_path).stem}_noisy.pkl\", \"wb\") as f:\n",
    "            pkl.dump(noisy_datasets, f)"
   ],
   "id": "545003100f92be37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ff44bb4d0d14968b876f43bb83e19fc"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2026-01-03T20:48:05.964316800Z",
     "start_time": "2026-01-02T21:24:03.850398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "all_datasets_results = defaultdict(dict)\n",
    "for exp in tqdm(os.listdir(r\"D:\\Projects\\trace-denoise\\final_runs_extended\")[10:]):\n",
    "    target_dir = os.path.join(r\"D:\\Projects\\trace-denoise\\final_runs_extended\", exp)\n",
    "    cfg = load_experiment_config(target_dir)\n",
    "    cfg.device = \"cuda:0\"\n",
    "    train_dataset, test_dataset, dk_process_model, dk_init_marking, dk_final_marking, rg_transition_matrix, diffuser, denoiser, final_res = load_experiment_data_and_model(target_dir, cfg)\n",
    "    base_test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "    gt, denoised, acc, pre, rec = evaluate_dataset_new(denoiser, diffuser, cfg, base_test_loader)\n",
    "    all_datasets_results[exp][\"base\"] = (gt, denoised, acc, pre, rec)\n",
    "    noisy_evals = []\n",
    "    with open(f\"{Path(cfg.data_path).stem}_noisy.pkl\", \"rb\") as f:\n",
    "        noisy_datasets = pkl.load(f)\n",
    "    with open(cfg.data_path, \"rb\") as f:\n",
    "        data = pkl.load(f)\n",
    "    target, source = data['target'], data['stochastic']\n",
    "    for noisy_ds in tqdm(noisy_datasets):\n",
    "        noisy_dataset = SaladsDataset(target, noisy_ds)\n",
    "        _, noisy_test_dataset = train_test_split(noisy_dataset, train_size=cfg.train_percent, random_state=cfg.seed)\n",
    "        noisy_test_loader = DataLoader(noisy_test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "        gt, denoised, acc, pre, rec = evaluate_dataset_new(denoiser, diffuser, cfg, noisy_test_loader)\n",
    "        noisy_evals.append((gt, denoised, acc, pre, rec))\n",
    "    all_datasets_results[exp][\"noisy\"] = noisy_evals\n",
    "    with open(r\"all_datasets_results.pkl\", \"wb\") as f:\n",
    "        pkl.dump(all_datasets_results, f)"
   ],
   "id": "107f7c52f1053100",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f99e58c227f4e11a09df0890f201b4d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc1b06b130b74da2b5caddc02333d553"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maxim\\anaconda3\\Lib\\site-packages\\torch_geometric\\utils\\convert.py:278: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  data_dict[key] = torch.as_tensor(value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed2831460f2646a3b8e19e8b3da20686"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbf5f853bb424cf59e0e81b37e2f025f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a233c0bfa2f4fc1946ddd5cac07b54e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0d63a1ecd6043f0914dcfe08ee79e45"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec625977bc7d4ce88263f40456ca315f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ebb41f1542d84a319bdb2874456b7340"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed5d692a3a564d049d8ec1c0028aa019"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8e513c35de6404d8226ffba691862eb"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0839c950957541079c15569d611b1a90"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T20:51:47.910426Z",
     "start_time": "2026-01-03T20:51:17.819009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"all_datasets_results.pkl\", \"rb\") as f:\n",
    "    all_datasets_results = pkl.load(f)"
   ],
   "id": "23828d497a89d72d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T20:53:34.980881Z",
     "start_time": "2026-01-03T20:53:34.976813Z"
    }
   },
   "cell_type": "code",
   "source": "{k: v['base'][2] for k, v in all_datasets_results.items()}",
   "id": "a222d8e4e32c868b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'50_salads_free': 0.8592291944696319,\n",
       " '50_salads_gnn_atn': 0.8880994210063227,\n",
       " '50_salads_matrix': 0.8673383070942174,\n",
       " 'ava_free': 0.959640197053902,\n",
       " 'ava_gnn': 0.9653844066668041,\n",
       " 'ava_matrix': 0.9619945467121948,\n",
       " 'bpi12_free': 0.9748650708975153,\n",
       " 'bpi12_gnn_atn': 0.9773906311301692,\n",
       " 'bpi12_matrix': 0.9786881008230166,\n",
       " 'breakfast_free': 0.9304494269260867}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T20:55:31.389972Z",
     "start_time": "2026-01-03T20:55:31.384078Z"
    }
   },
   "cell_type": "code",
   "source": "os.listdir(r\"D:\\Projects\\trace-denoise\\final_runs_extended\")[10:]",
   "id": "f1b6023923c3eb24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['breakfast_gnn_atn',\n",
       " 'breakfast_matrix',\n",
       " 'gtea_free',\n",
       " 'gtea_gnn_atn',\n",
       " 'gtea_matrix']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T21:01:15.406051Z",
     "start_time": "2026-01-03T21:01:15.402661Z"
    }
   },
   "cell_type": "code",
   "source": "temp = \"../data/pickles/50_salads_unified.pkl\"",
   "id": "f33fbdbb8c44273e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T21:01:58.789420Z",
     "start_time": "2026-01-03T21:01:58.785201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(temp).stem"
   ],
   "id": "b36eccfa48eb9bad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50_salads_unified'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b2b31a41553854ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
